<?xml version="1.0" encoding="utf-8" ?>
<article>
  <articleinfo>
    <title>hebbRNN: A Reward-Modulated Hebbian Learning Rule for Recurrent Neural Networks</title>
    <authors>
      <author>
        <name>Jonathan A Michaels</name>
        <orcid>0000-0002-5179-3181</orcid>
        <affiliation>
          <orgname>
            German Primate Center, Göttingen, Germany
          </orgname>
        </affiliation>
      </author>
      <author>
        <name>Hansjörg Scherberger</name>
        <orcid>0000-0001-6593-2800</orcid>
        <affiliation>
          <orgname>
            German Primate Center, Göttingen, Germany; Biology Department, University of Göttingen, Germany
          </orgname>
        </affiliation>
      </author>
    </authors>
    <tags>
      <tag>learning</tag>
      <tag>plasticity</tag>
      <tag>neural network</tag>
      <tag>Hebbian</tag>
      <tag>RNN</tag>
    </tags>
    <date>22 August 2016</date>
    <paper_doi>10.21105/joss.00060</paper_doi>
    <software_repository>https://github.com/JonathanAMichaels/hebbRNN</software_repository>
    <software_archive>http://dx.doi.org/10.5281/zenodo.154745</software_archive>
    <paper_url>https://github.com/openjournals/joss-papers/blob/master/joss.00060/10.21105.joss.00060.pdf</paper_url>
  </articleinfo>
  <body>
    <h1 id="summary">Summary</h1>
    <p>How does our brain learn to produce the large, impressive, and flexible array of motor behaviors we possess? In recent years, there has been renewed interest in modeling complex human behaviors such as memory and motor skills using neural networks <span class="citation">(Sussillo et al. 2015; Rajan, Harvey, and Tank 2016; Hennequin, Vogels, and Gerstner 2014; Carnevale et al. 2015; Laje, Buonomano, and Buonomano 2013)</span>. However, training these networks to produce meaningful behavior has proven difficult. Furthermore, the most common methods are generally not biologically-plausible and rely on information not local to the synapses of individual neurons as well as instantaneous reward signals <span class="citation">(Martens and Sutskever 2011; Sussillo and Abbott 2009; Song, Yang, and Wang 2016)</span>.</p>
    <p>The current package is a Matlab implementation of a biologically-plausible training rule for recurrent neural networks using a delayed and sparse reward signal <span class="citation">(Miconi 2016)</span>. On individual trials, input is perturbed randomly at the synapses of individual neurons and these potential weight changes are accumulated in a Hebbian manner (multiplying pre- and post-synaptic weights) in an eligibility trace. At the end of each trial, a reward signal is determined based on the overall performance of the network in achieving the desired goal, and this reward is compared to the expected reward. The difference between the observed and expected reward is used in combination with the eligibility trace to strengthen or weaken corresponding synapses within the network, leading to proper network performance over time.</p>
    <h1 id="references" class="unnumbered">References</h1>
    <div id="refs" class="references">
    <div id="ref-Carnevale:2015jk">
    <p>Carnevale, Federico, Victor de Lafuente, Ranulfo Romo, Omri Barak, and Néstor Parga. 2015. “Dynamic Control of Response Criterion in Premotor Cortex during Perceptual Detection under Temporal Uncertainty.” <em>Neuron</em> 86 (4): 1067–77. doi:<a href="https://doi.org/10.1016/j.neuron.2015.04.014">10.1016/j.neuron.2015.04.014</a>.</p>
    </div>
    <div id="ref-Hennequin:2014jh">
    <p>Hennequin, Guillaume, Tim P Vogels, and Wulfram Gerstner. 2014. “Optimal control of transient dynamics in balanced networks supports generation of complex movements.” <em>Neuron</em> 82 (6): 1394–1406. doi:<a href="https://doi.org/10.1016/j.neuron.2014.04.045">10.1016/j.neuron.2014.04.045</a>.</p>
    </div>
    <div id="ref-Laje:2013bd">
    <p>Laje, Rodrigo, Dean V Buonomano, and Dean V Buonomano. 2013. “Robust timing and motor patterns by taming chaos in recurrent neural networks.” <em>Nature Neuroscience</em> 16 (7): 925–33. doi:<a href="https://doi.org/10.1038/nn.3405">10.1038/nn.3405</a>.</p>
    </div>
    <div id="ref-Martens:2011vh">
    <p>Martens, J, and I Sutskever. 2011. “Learning recurrent neural networks with hessian-free optimization.” <em>Proceedings of the 28th International Conference on Machine Learning</em>.</p>
    </div>
    <div id="ref-Miconi:2016dja">
    <p>Miconi, Thomas. 2016. “Flexible decision-making in recurrent neural networks trained with a biologically plausible rule.” <em>BioRxiv</em>, July. doi:<a href="https://doi.org/10.1101/057729">10.1101/057729</a>.</p>
    </div>
    <div id="ref-Rajan:2016cp">
    <p>Rajan, Kanaka, Christopher D Harvey, and David W Tank. 2016. “Recurrent Network Models of Sequence Generation and Memory.” <em>Neuron</em> 90 (1): 128–42. doi:<a href="https://doi.org/10.1016/j.neuron.2016.02.009">10.1016/j.neuron.2016.02.009</a>.</p>
    </div>
    <div id="ref-Song:2016fja">
    <p>Song, H Francis, Guangyu R Yang, and Xiao-Jing Wang. 2016. “Training Excitatory-Inhibitory Recurrent Neural Networks for Cognitive Tasks: A Simple and Flexible Framework.” <em>PLoS Computational Biology</em> 12 (2): e1004792. doi:<a href="https://doi.org/10.1371/journal.pcbi.1004792">10.1371/journal.pcbi.1004792</a>.</p>
    </div>
    <div id="ref-Sussillo:2009gh">
    <p>Sussillo, David, and L F Abbott. 2009. “Generating coherent patterns of activity from chaotic neural networks.” <em>Neuron</em> 63 (4): 544–57. doi:<a href="https://doi.org/10.1016/j.neuron.2009.07.018">10.1016/j.neuron.2009.07.018</a>.</p>
    </div>
    <div id="ref-Sussillo:2015kp">
    <p>Sussillo, David, Mark M Churchland, Matthew T Kaufman, and Krishna V Shenoy. 2015. “A neural network that finds a naturalistic solution for the production of muscle activity.” <em>Nature Neuroscience</em> 18 (7): 1025–33. doi:<a href="https://doi.org/10.1038/nn.4042">10.1038/nn.4042</a>.</p>
    </div>
    </div>
  </body>
</article>
