<?xml version="1.0" encoding="utf-8" ?>
<article>
  <articleinfo>
    <title>schwimmbad: A uniform interface to parallel processing pools in Python</title>
    <authors>
      <author>
        <name>Adrian M. Price-Whelan</name>
        <orcid>0000-0003-0872-7098</orcid>
        <affiliation>
          <orgname>
            1
          </orgname>
        </affiliation>
      </author>
      <author>
        <name>Daniel Foreman-Mackey</name>
        <orcid>0000-0002-9328-5652</orcid>
        <affiliation>
          <orgname>
            2
          </orgname>
        </affiliation>
      </author>
    </authors>
    <tags>
      <tag>Python</tag>
      <tag>multiprocessing</tag>
      <tag>parallel computing</tag>
    </tags>
    <date>11 August 2017</date>
    <paper_doi>10.21105/joss.00357</paper_doi>
    <software_repository>https://github.com/adrn/schwimmbad</software_repository>
    <software_archive>http://dx.doi.org/10.5281/zenodo.885577</software_archive>
    <paper_url>https://github.com/openjournals/joss-papers/raw/master/joss.00357/10.21105.joss.00357.pdf</paper_url>
  </articleinfo>
  <body>
    <h1 id="summary">Summary</h1>
    <p>Many scientific and computing problems require doing some calculation on all elements of some data set. If the calculations can be executed in parallel (i.e. without any communication between calculations), these problems are said to be <a href="https://en.wikipedia.org/wiki/Embarrassingly_parallel"><em>perfectly parallel</em></a>. On computers with multiple processing cores, these tasks can be distributed and executed in parallel to greatly improve performance. A common paradigm for handling these distributed computing problems is to use a processing &quot;pool&quot;: the &quot;tasks&quot; (the data) are passed in bulk to the pool, and the pool handles distributing the tasks to a number of worker processes when available.</p>
    <p>In Python, the built-in <code>multiprocessing</code> package provides a <code>Pool</code> class for exactly this design case, but only supports distributing the tasks amongst multiple cores of a single processor. To extend to large cluster computing environments, other protocols are required, such as the Message Passing Interface <span class="citation">(MPI; Forum 1994)</span>. <code>schwimmbad</code> provides new <code>Pool</code> classes for a number of parallel processing environments with a consistent interface. This enables easily switching between local development (e.g., serial processing or with Python's built-in <code>multiprocessing</code>) and deployment on a cluster or supercomputer (via, e.g., MPI or JobLib). This library supports processing pools with a number of backends:</p>
    <ul>
    <li>Serial processing: <code>SerialPool</code></li>
    <li><code>Python</code> standard-library <code>multiprocessing</code>: <code>MultiPool</code></li>
    <li><a href="open-mpi.org"><code>OpenMPI</code></a> <span class="citation">(Gabriel et al. 2004)</span> and <a href="https://www.mpich.org/"><code>mpich2</code></a> <span class="citation">(Lusk, Doss, and Skjellum 1996)</span> via the <code>mpi4py</code> package <span class="citation">(Dalcín, Paz, and Storti 2005; Dalcín et al. 2008)</span>: <code>MPIPool</code></li>
    <li><a href="http://pythonhosted.org/joblib/"><code>joblib</code></a>: <code>JoblibPool</code></li>
    </ul>
    <p>All pool classes provide a <code>.map()</code> method to distribute tasks to a specified worker function (or callable), and support specifying a callback function that is executed on the master process to enable post-processing or caching the results as they are delivered.</p>
    <h1 id="references" class="unnumbered">References</h1>
    <div id="refs" class="references">
    <div id="ref-Dalcin2005">
    <p>Dalcín, Lisandro, Rodrigo Paz, and Mario Storti. 2005. “MPI for Python.” <em>Journal of Parallel and Distributed Computing</em> 65 (9): 1108–15. doi:<a href="https://doi.org/http://dx.doi.org/10.1016/j.jpdc.2005.03.010">http://dx.doi.org/10.1016/j.jpdc.2005.03.010</a>.</p>
    </div>
    <div id="ref-Dalcin2008">
    <p>Dalcín, Lisandro, Rodrigo Paz, Mario Storti, and Jorge D’Elía. 2008. “MPI for Python: Performance Improvements and Mpi-2 Extensions.” <em>Journal of Parallel and Distributed Computing</em> 68 (5): 655–62. doi:<a href="https://doi.org/http://dx.doi.org/10.1016/j.jpdc.2007.09.005">http://dx.doi.org/10.1016/j.jpdc.2007.09.005</a>.</p>
    </div>
    <div id="ref-Forum1994">
    <p>Forum, Message P. 1994. “MPI: A Message-Passing Interface Standard.” Knoxville, TN, USA: University of Tennessee.</p>
    </div>
    <div id="ref-Gabriel2004">
    <p>Gabriel, Edgar, Graham E. Fagg, George Bosilca, Thara Angskun, Jack J. Dongarra, Jeffrey M. Squyres, Vishal Sahay, et al. 2004. “Open MPI: Goals, Concept, and Design of a Next Generation MPI Implementation.” In <em>Proceedings, 11th European Pvm/Mpi Users’ Group Meeting</em>, 97–104. Budapest, Hungary.</p>
    </div>
    <div id="ref-Lusk1996">
    <p>Lusk, Ewing, Nathan Doss, and Anthony Skjellum. 1996. “A High-Performance, Portable Implementation of the Mpi Message Passing Interface Standard.” <em>Parallel Computing</em> 22: 789–828.</p>
    </div>
    </div>
  </body>
</article>
