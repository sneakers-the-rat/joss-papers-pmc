<?xml version="1.0" encoding="utf-8" ?>
<article>
  <articleinfo>
    <title>Philentropy: Information Theory and Distance Quantification with R</title>
    <authors>
      <author>
        <name>Hajk-Georg Drost</name>
        <orcid>0000-0002-1567-306X</orcid>
        <affiliation>
          <orgname>
            1
          </orgname>
        </affiliation>
      </author>
    </authors>
    <tags>
      <tag>R</tag>
      <tag>information theory</tag>
      <tag>distance metrics</tag>
      <tag>probability functions</tag>
      <tag>divergence quantification</tag>
      <tag>jensen-shannon divergence</tag>
    </tags>
    <date>22 May 2018</date>
    <paper_doi>10.21105/joss.00765</paper_doi>
    <software_repository>https://github.com/HajkD/philentropy</software_repository>
    <software_archive>http://dx.doi.org/10.5281/zenodo.1286221</software_archive>
    <paper_url>http://www.theoj.org/joss-papers/joss.00765/10.21105.joss.00765.pdf</paper_url>
  </articleinfo>
  <body>
    <h1 id="summary">Summary</h1>
    <p>Comparison is a fundamental method of scientific research leading to insights about the processes that generate similarity or dissimilarity. In statistical terms comparisons between probability functions are performed to infer connections, correlations, or relationships between objects or samples <span class="citation" data-cites="Cha2007">(Cha 2007)</span>. Most quantification methods rely on distance or similarity measures, but the right choice for each individual application is not always clear and sometimes poorly explored. The reason for this is partly that diverse measures are either implemented in different R packages with very different notations or are not implemented at all. Thus, a comprehensive framework implementing the most common similarity and distance measures using a uniform notation is still missing. The R <span class="citation" data-cites="R2018">(R Core Team 2018)</span> package <code>Philentropy</code> aims to fill this gap by implementing forty-six fundamental distance and similarity measures <span class="citation" data-cites="Cha2007">(Cha 2007)</span> for comparing probability functions. These comparisons between probability functions have their foundations in a broad range of scientific disciplines from mathematics to ecology. The aim of this package is to provide a comprehensive and computationally optimized base framework for clustering, classification, statistical inference, goodness-of-fit, non-parametric statistics, information theory, and machine learning tasks that are based on comparing univariate or multivariate probability functions. All functions are written in C++ and are integrated into the R package using the Rcpp Application Programming Interface (API) <span class="citation" data-cites="Eddelbuettel2013">(Eddelbuettel 2013)</span>.</p>
    <p>Together, this framework allows building new similarity or distance based (statistical) models and algorithms in R which are computationally efficient and scalable. The comprehensive availability of diverse metrics and measures furthermore enables a systematic assessment of choosing the most optimal similarity or distance measure for individual applications in diverse scientific disciplines.</p>
    <p>The following probability distance/similarity and information theory measures are implemented in <code>Philentropy</code>.</p>
    <h3 id="distance-and-similarity-measures">Distance and Similarity Measures</h3>
    <h4 id="l_p-minkowski-family"><span class="math inline"><em>L</em><sub><em>p</em></sub></span> Minkowski Family</h4>
    <ul>
    <li>Euclidean : <span class="math inline">$d = \sqrt{\sum_{i = 1}^N | P_i - Q_i |^2)}$</span></li>
    <li>Manhattan : <span class="math inline">$d = \sum_{i = 1}^N | P_i - Q_i |$</span></li>
    <li>Minkowski : <span class="math inline">$d = ( \sum_{i = 1}^N | P_i - Q_i |^p)^{1/p}$</span></li>
    <li>Chebyshev : <span class="math inline"><em>d</em> = <em>m</em><em>a</em><em>x</em>|<em>P</em><sub><em>i</em></sub> − <em>Q</em><sub><em>i</em></sub>|</span></li>
    </ul>
    <h4 id="l_1-family"><span class="math inline"><em>L</em><sub>1</sub></span> Family</h4>
    <ul>
    <li>Sorensen : <span class="math inline">$d = \frac{\sum_{i = 1}^N | P_i - Q_i |}{\sum_{i = 1}^N (P_i + Q_i)}$</span></li>
    <li>Gower : <span class="math inline">$d = \frac{1}{N} \dot \sum_{i = 1}^N | P_i - Q_i |$</span>, where <span class="math inline"><em>N</em></span> is the total number of elements <span class="math inline"><em>i</em></span> in <span class="math inline"><em>P</em><sub><em>i</em></sub></span> and <span class="math inline"><em>Q</em><sub><em>i</em></sub></span></li>
    <li>Soergel : <span class="math inline">$d = \frac{\sum_{i = 1}^N | P_i - Q_i |}{\sum_{i = 1}^N max(P_i , Q_i)}$</span></li>
    <li>Kulczynski d : <span class="math inline">$d = \frac{\sum_{i = 1}^N | P_i - Q_i |}{\sum_{i = 1}^N min(P_i , Q_i)}$</span></li>
    <li>Canberra : <span class="math inline">$d = \frac{\sum_{i = 1}^N | P_i - Q_i |}{(P_i + Q_i)}$</span></li>
    <li>Lorentzian : <span class="math inline">$d = \sum_{i = 1}^N ln(1 + | P_i - Q_i |)$</span></li>
    </ul>
    <h4 id="intersection-family">Intersection Family</h4>
    <ul>
    <li>Intersection : <span class="math inline">$s = \sum_{i = 1}^N min(P_i , Q_i)$</span></li>
    <li>Non-Intersection : <span class="math inline">$d = 1 - \sum_{i = 1}^N min(P_i , Q_i)$</span></li>
    <li>Wave Hedges : <span class="math inline">$d = \frac{\sum_{i = 1}^N | P_i - Q_i |}{max(P_i , Q_i)}$</span></li>
    <li>Czekanowski : <span class="math inline">$d = \frac{\sum_{i = 1}^N | P_i - Q_i |}{\sum_{i = 1}^N | P_i + Q_i |}$</span></li>
    <li>Motyka : <span class="math inline">$d = \frac{\sum_{i = 1}^N min(P_i , Q_i)}{(P_i + Q_i)}$</span></li>
    <li>Kulczynski s : <span class="math inline">$d = \frac{\sum_{i = 1}^N min(P_i , Q_i)}{\sum_{i = 1}^N | P_i - Q_i |}$</span></li>
    <li>Tanimoto : <span class="math inline">$d = \frac{\sum_{i = 1}^N (max(P_i , Q_i) - min(P_i , Q_i))}{\sum_{i = 1}^N max(P_i , Q_i)}$</span> ; equivalent to Soergel</li>
    <li>Ruzicka : <span class="math inline">$s = \frac{\sum_{i = 1}^N min(P_i , Q_i)}{\sum_{i = 1}^N max(P_i , Q_i)}$</span> ; equivalent to 1 - Tanimoto = 1 - Soergel</li>
    </ul>
    <h4 id="inner-product-family">Inner Product Family</h4>
    <ul>
    <li>Inner Product : <span class="math inline">$s = \sum_{i = 1}^N P_i \dot Q_i$</span></li>
    <li>Harmonic mean : <span class="math inline">$s = 2 \cdot \frac{ \sum_{i = 1}^N P_i \cdot Q_i}{P_i + Q_i}$</span></li>
    <li>Cosine : <span class="math inline">$s = \frac{\sum_{i = 1}^N P_i \cdot Q_i}{\sqrt{\sum_{i = 1}^N P_i^2} \cdot \sqrt{\sum_{i = 1}^N Q_i^2}}$</span></li>
    <li>Kumar-Hassebrook (PCE) : <span class="math inline">$s = \frac{\sum_{i = 1}^N (P_i \cdot Q_i)}{(\sum_{i = 1}^N P_i^2 + \sum_{i = 1}^N Q_i^2 - \sum_{i = 1}^N (P_i \cdot Q_i))}$</span></li>
    <li>Jaccard : <span class="math inline">$d = 1 - \frac{\sum_{i = 1}^N P_i \cdot Q_i}{\sum_{i = 1}^N P_i^2 + \sum_{i = 1}^N Q_i^2 - \sum_{i = 1}^N P_i \cdot Q_i}$</span> ; equivalent to 1 - Kumar-Hassebrook</li>
    <li>Dice : <span class="math inline">$d = \frac{\sum_{i = 1}^N (P_i - Q_i)^2}{(\sum_{i = 1}^N P_i^2 + \sum_{i = 1}^N Q_i^2)}$</span></li>
    </ul>
    <h4 id="squared-chord-family">Squared-chord Family</h4>
    <ul>
    <li>Fidelity : <span class="math inline">$s = \sum_{i = 1}^N \sqrt{P_i \cdot Q_i}$</span></li>
    <li>Bhattacharyya : <span class="math inline">$d = - ln \sum_{i = 1}^N \sqrt{P_i \cdot Q_i}$</span></li>
    <li>Hellinger : <span class="math inline">$d = 2 \cdot \sqrt{1 - \sum_{i = 1}^N \sqrt{P_i \cdot Q_i}}$</span></li>
    <li>Matusita : <span class="math inline">$d = \sqrt{2 - 2 \cdot \sum_{i = 1}^N \sqrt{P_i \cdot Q_i}}$</span></li>
    <li>Squared-chord : <span class="math inline">$d = \sum_{i = 1}^N ( \sqrt{P_i} - \sqrt{Q_i} )^2$</span></li>
    </ul>
    <h4 id="squared-l_2-family-x2-squared-family">Squared <span class="math inline"><em>L</em><sub>2</sub></span> family (<span class="math inline"><em>X</em><sup>2</sup></span> squared family)</h4>
    <ul>
    <li>Squared Euclidean : <span class="math inline">$d = \sum_{i = 1}^N ( P_i - Q_i )^2$</span></li>
    <li>Pearson <span class="math inline"><em>X</em><sup>2</sup></span> : <span class="math inline">$d = \sum_{i = 1}^N ( \frac{(P_i - Q_i )^2}{Q_i} )$</span></li>
    <li>Neyman <span class="math inline"><em>X</em><sup>2</sup></span> : <span class="math inline">$d = \sum_{i = 1}^N ( \frac{(P_i - Q_i )^2}{P_i} )$</span></li>
    <li>Squared <span class="math inline"><em>X</em><sup>2</sup></span> : <span class="math inline">$d = \sum_{i = 1}^N ( \frac{(P_i - Q_i )^2}{(P_i + Q_i)} )$</span></li>
    <li>Probabilistic Symmetric <span class="math inline"><em>X</em><sup>2</sup></span> : <span class="math inline">$d = 2 \cdot \sum_{i = 1}^N ( \frac{(P_i - Q_i )^2}{(P_i + Q_i)} )$</span></li>
    <li>Divergence : <span class="math inline"><em>X</em><sup>2</sup></span> : <span class="math inline">$d = 2 \cdot \sum_{i = 1}^N ( \frac{(P_i - Q_i )^2}{(P_i + Q_i)^2} )$</span></li>
    <li>Clark : <span class="math inline">$d = \sqrt{\sum_{i = 1}^N (\frac{| P_i - Q_i |}{(P_i + Q_i)^2}}$</span></li>
    <li>Additive Symmetric <span class="math inline"><em>X</em><sup>2</sup></span> : <span class="math inline">$d = \sum_{i = 1}^N ( \frac{((P_i - Q_i)^2 \cdot (P_i + Q_i))}{(P_i \cdot Q_i)} )$</span></li>
    </ul>
    <h4 id="shannons-entropy-family">Shannon’s Entropy Family</h4>
    <ul>
    <li>Kullback-Leibler : <span class="math inline">$d = \sum_{i = 1}^N P_i \cdot log(\frac{P_i}{Q_i})$</span></li>
    <li>Jeffreys : <span class="math inline">$d = \sum_{i = 1}^N (P_i - Q_i) \cdot log(\frac{P_i}{Q_i})$</span></li>
    <li>K divergence : <span class="math inline">$d = \sum_{i = 1}^N P_i \cdot log(\frac{2 \cdot P_i}{P_i + Q_i})$</span></li>
    <li>Topsoe : <span class="math inline">$d = \sum_{i = 1}^N ( P_i \cdot log(\frac{2 \cdot P_i}{P_i + Q_i}) ) + ( Q_i \cdot log(\frac{2 \cdot Q_i}{P_i + Q_i}) )$</span></li>
    <li>Jensen-Shannon : <span class="math inline">$d = 0.5 \cdot ( \sum_{i = 1}^N P_i \cdot log(\frac{2 \cdot P_i}{P_i + Q_i}) + \sum_{i = 1}^N Q_i \cdot log(\frac{2 * Q_i}{P_i + Q_i}))$</span></li>
    <li>Jensen difference : <span class="math inline">$d = \sum_{i = 1}^N ( (\frac{P_i \cdot log(P_i) + Q_i \cdot log(Q_i)}{2}) - (\frac{P_i + Q_i}{2}) \cdot log(\frac{P_i + Q_i}{2}) )$</span></li>
    </ul>
    <h4 id="combinations">Combinations</h4>
    <ul>
    <li>Taneja : <span class="math inline">$d = \sum_{i = 1}^N ( \frac{P_i + Q_i}{2}) \cdot log( \frac{P_i + Q_i}{( 2 \cdot \sqrt{P_i \cdot Q_i})} )$</span></li>
    <li>Kumar-Johnson : <span class="math inline">$d = \sum_{i = 1}^N \frac{(P_i^2 - Q_i^2)^2}{2 \cdot (P_i \cdot Q_i)^{\frac{3}{2}}}$</span></li>
    <li>Avg(<span class="math inline"><em>L</em><sub>1</sub></span>, <span class="math inline"><em>L</em><sub><em>n</em></sub></span>) : <span class="math inline">$d = \frac{\sum_{i = 1}^N | P_i - Q_i| + max{ | P_i - Q_i |}}{2}$</span></li>
    </ul>
    <p><strong>Note</strong>: <span class="math inline"><em>d</em></span> refers to distance measures, whereas <span class="math inline"><em>s</em></span> denotes similarity measures.</p>
    <h3 id="information-theory-measures">Information Theory Measures</h3>
    <ul>
    <li>Shannon’s Entropy H(X) : <span class="math inline">$H(X) = -\sum\limits_{i=1}^n P(x_i) \cdot log_b(P(x_i))$</span></li>
    <li>Shannon’s Joint-Entropy H(X,Y) : <span class="math inline">$H(X,Y) = -\sum\limits_{i=1}^n\sum\limits_{j=1}^m P(x_i, y_j) \cdot log_b(P(x_i, y_j))$</span></li>
    <li>Shannon’s Conditional-Entropy H(X | Y) : <span class="math inline">$H(Y|X) = \sum\limits_{i=1}^n\sum\limits_{j=1}^m P(x_i, y_j) \cdot log_b( \frac{P(x_i)}{P(x_i, y_j)})$</span></li>
    <li>Mutual Information I(X,Y) : <span class="math inline">$MI(X,Y) = \sum\limits_{i=1}^n\sum\limits_{j=1}^m P(x_i, y_j) \cdot log_b( \frac{P(x_i, y_j)}{( P(x_i) * P(y_j) )})$</span></li>
    <li>Kullback-Leibler Divergence : <span class="math inline">$KL(P || Q) = \sum\limits_{i=1}^n P(p_i) \cdot log_2(\frac{P(p_i) }{P(q_i)}) = H(P, Q) - H(P)$</span></li>
    <li>Jensen-Shannon Divergence : <span class="math inline"><em>J</em><em>S</em><em>D</em>(<em>P</em>||<em>Q</em>)=0.5 * (<em>K</em><em>L</em>(<em>P</em>||<em>R</em>)+<em>K</em><em>L</em>(<em>Q</em>||<em>R</em>))</span></li>
    <li>Generalized Jensen-Shannon Divergence : <span class="math inline">$gJSD_{\pi_1,...,\pi_n}(P_1, ..., P_n) = H(\sum_{i = 1}^n \pi_i \cdot P_i) - \sum_{i = 1}^n \pi_i \cdot H(P_i)$</span></li>
    </ul>
    <p><code>Philentropy</code> already enabled the robust comparison of similarity measures in analogy-based software effort estimation <span class="citation" data-cites="Phannachitta2017">(Phannachitta 2017)</span> as well as in evolutionary transcriptomics applications <span class="citation" data-cites="Drost2018">(Drost et al. 2018)</span>. The package aims to assist efforts to determine optimal similarity or distance measures when developing new (statistical) models or algorithms. In addition, <code>Philentropy</code> is implemented to be applicable to large-scale datasets that were previously inaccessible using other R packages. The software is open source and currently available on GitHub (https://github.com/HajkD/philentropy) and CRAN (https://cran.r-project.org/web/packages/philentropy/index.html). A comprehensive documentation of <code>Philentropy</code> can be found at https://hajkd.github.io/philentropy/.</p>
    <h1 id="acknowledgements">Acknowledgements</h1>
    <p>I would like to thank Jerzy Paszkowski for providing me an inspiring scientific environment and for supporting my projects.</p>
    <h1 id="references" class="unnumbered">References</h1>
    <div id="refs" class="references">
    <div id="ref-Cha2007">
    <p>Cha, Sung-Hyuk. 2007. “Comprehensive Survey on Distance/Similarity Measures Between Probability Density Functions.” <em>International Journal of Mathematical Models and Methods in Applied Science</em> 1 (4):300–307.</p>
    </div>
    <div id="ref-Drost2018">
    <p>Drost, Hajk-Georg, Alexander Gabel, Jialin Liu, Marcel Quint, and Ivo Grosse. 2018. “MyTAI: Evolutionary Transcriptomics with R.” <em>Bioinformatics</em> 34 (9):1589–90. <a href="https://doi.org/10.1093/bioinformatics/btx835" class="uri">https://doi.org/10.1093/bioinformatics/btx835</a>.</p>
    </div>
    <div id="ref-Eddelbuettel2013">
    <p>Eddelbuettel, Dirk. 2013. <em>Seamless R and C++ Integration with Rcpp</em>. Use R! New York: Springer.</p>
    </div>
    <div id="ref-Phannachitta2017">
    <p>Phannachitta, P. 2017. “Robust Comparison of Similarity Measures in Analogy Based Software Effort Estimation.” In <em>2017 11th International Conference on Software, Knowledge, Information Management and Applications (Skima)</em>, 1–7. <a href="https://doi.org/10.1109/SKIMA.2017.8294126" class="uri">https://doi.org/10.1109/SKIMA.2017.8294126</a>.</p>
    </div>
    <div id="ref-R2018">
    <p>R Core Team. 2018. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/" class="uri">https://www.R-project.org/</a>.</p>
    </div>
    </div>
  </body>
</article>
