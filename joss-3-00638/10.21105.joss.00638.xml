<?xml version="1.0" encoding="utf-8" ?>
<article>
  <articleinfo>
    <title>MLxtend: Providing machine learning and data science utilities and extensions to Python’s scientific computing stack</title>
    <authors>
      <author>
        <name>Sebastian Raschka</name>
        <orcid>0000-0001-6989-4493</orcid>
        <affiliation>
          <orgname>
            1
          </orgname>
        </affiliation>
      </author>
    </authors>
    <tags>
      <tag>machine learning</tag>
      <tag>data science</tag>
      <tag>association rule mining</tag>
      <tag>ensemble learning</tag>
      <tag>feature selection</tag>
    </tags>
    <date>15 March 2018</date>
    <paper_doi>10.21105/joss.00638</paper_doi>
    <software_repository>https://github.com/rasbt/mlxtend</software_repository>
    <software_archive>http://dx.doi.org/10.5281/zenodo.1226560</software_archive>
    <paper_url>http://www.theoj.org/joss-papers/joss.00638/10.21105.joss.00638.pdf</paper_url>
  </articleinfo>
  <body>
    <h1 id="summary">Summary</h1>
    <p>MLxtend is a library that implements a variety of core algorithms and utilities for machine learning and data mining. The primary goal of MLxtend is to make commonly used tools accessible to researchers in academia and data scientists in industries focussing on user-friendly and intuitive APIs and compatibility to existing machine learning libraries, such as scikit-learn, when appropriate. While MLxtend implements a large variety of functions, highlights include sequential feature selection algorithms <span class="citation" data-cites="pudil1994floating">(Pudil, Novovičová, and Kittler 1994)</span>, implementations of stacked generalization <span class="citation" data-cites="wolpert1992stacked">(Wolpert 1992)</span> for classification and regression, and algorithms for frequent pattern mining <span class="citation" data-cites="agrawal1994fast">(Agrawal and Ramakrishnan 1994)</span>. The sequential feature selection algorithms cover forward, backward, forward floating, and backward floating selection and leverage scikit-learn’s cross-validation API <span class="citation" data-cites="pedregosa2011scikit">(Pedregosa et al. 2011)</span> to ensure satisfactory generalization performance upon constructing and selecting feature subsets. Besides, visualization functions are provided that allow users to inspect the estimated predictive performance, including performance intervals, for different feature subsets. The ensemble methods in MLxtend cover majority voting, stacking, and stacked generalization, all of which are compatible with scikit-learn estimators and other libraries as XGBoost <span class="citation" data-cites="chen2016xgboost">(Chen and Guestrin 2016)</span>. In addition to feature selection, classification, and regression algorithms, MLxtend implements model evaluation techniques for comparing the performance of two different models via McNemar’s test and multiple models via Cochran’s Q test. An implementation of the 5x2 cross-validated paired t-test <span class="citation" data-cites="dietterich1998approximate">(Dietterich 1998)</span> allows users to compare the performance of machine learning algorithms to each other. Furthermore, different flavors of the Bootstrap method <span class="citation" data-cites="efron1994introduction">(Efron and Tibshirani 1994)</span>, such as the .632 Bootstrap method <span class="citation" data-cites="efron1983estimating">(Efron 1983)</span> are implemented to compute confidence intervals of performance estimates. All in all, MLxtend provides a large variety of different utilities that build upon and extend the capabilities of Python’s scientific computing stack.</p>
    <h1 id="acknowledgements">Acknowledgements</h1>
    <p>I would like to acknowledge all of the contributors and users of mlxtend, who helped with valuable feedback, bug fixes, and additional functionality to further improve the library: James Bourbeau, Reiichiro Nakano, Will McGinnis, Guillaume Poirier-Morency, Colin Carrol, Zach Griffith, Anton Loss, Joshua Goerner, Eike Dehling, Gilles Armand, Adam Erickson, Mathew Savage, Pablo Fernandez, Alejandro Correa Bahnsen, and many others. A comprehensive list of all contributors to mlxtend is available at https://github.com/rasbt/mlxtend/graphs/contributors.</p>
    <h1 id="references" class="unnumbered">References</h1>
    <div id="refs" class="references">
    <div id="ref-agrawal1994fast">
    <p>Agrawal, Rakesh, and Srikant Ramakrishnan. 1994. “Fast Algorithms for Mining Association Rules in Large Databases.” In <em>VLDB’94, Proceedings of 20th International Conference on Very Large Data Bases, September 12-15, 1994, Santiago de Chile, Chile</em>, 1215:487–99.</p>
    </div>
    <div id="ref-chen2016xgboost">
    <p>Chen, Tianqi, and Carlos Guestrin. 2016. “Xgboost: A Scalable Tree Boosting System.” In <em>Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining</em>, 785–94. ACM. <a href="https://doi.org/10.1145/2939672.2939785" class="uri">https://doi.org/10.1145/2939672.2939785</a>.</p>
    </div>
    <div id="ref-dietterich1998approximate">
    <p>Dietterich, Thomas G. 1998. “Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms.” <em>Neural Computation</em> 10 (7). MIT Press:1895–1923. <a href="https://doi.org/10.1162/089976698300017197" class="uri">https://doi.org/10.1162/089976698300017197</a>.</p>
    </div>
    <div id="ref-efron1983estimating">
    <p>Efron, Bradley. 1983. “Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation.” <em>Journal of the American Statistical Association</em> 78 (382). Taylor &amp; Francis Group:316–31. <a href="https://doi.org/10.1080/01621459.1983.10477973" class="uri">https://doi.org/10.1080/01621459.1983.10477973</a>.</p>
    </div>
    <div id="ref-efron1994introduction">
    <p>Efron, Bradley, and Robert J Tibshirani. 1994. <em>An Introduction to the Bootstrap</em>. CRC press.</p>
    </div>
    <div id="ref-pedregosa2011scikit">
    <p>Pedregosa, Fabian, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” <em>Journal of Machine Learning Research</em> 12 (Oct):2825–30.</p>
    </div>
    <div id="ref-pudil1994floating">
    <p>Pudil, Pavel, Jana Novovičová, and Josef Kittler. 1994. “Floating Search Methods in Feature Selection.” <em>Pattern Recognition Letters</em> 15 (11). Elsevier:1119–25. <a href="https://doi.org/https://doi.org/10.1016/0167-8655(94)90127-9" class="uri">https://doi.org/https://doi.org/10.1016/0167-8655(94)90127-9</a>.</p>
    </div>
    <div id="ref-wolpert1992stacked">
    <p>Wolpert, David H. 1992. “Stacked Generalization.” <em>Neural Networks</em> 5 (2). Elsevier:241–59. <a href="https://doi.org/https://doi.org/10.1016/S0893-6080(05)80023-1" class="uri">https://doi.org/https://doi.org/10.1016/S0893-6080(05)80023-1</a>.</p>
    </div>
    </div>
  </body>
</article>
