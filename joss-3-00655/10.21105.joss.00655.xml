<?xml version="1.0" encoding="utf-8" ?>
<article>
  <articleinfo>
    <title>Fast, Consistent Tokenization of Natural Language Text</title>
    <authors>
      <author>
        <name>Lincoln A. Mullen</name>
        <orcid>0000-0001-5103-6917</orcid>
        <affiliation>
          <orgname>
            1
          </orgname>
        </affiliation>
      </author>
      <author>
        <name>Kenneth Benoit</name>
        <orcid>0000-0002-0797-564X</orcid>
        <affiliation>
          <orgname>
            2
          </orgname>
        </affiliation>
      </author>
      <author>
        <name>Os Keyes</name>
        <orcid>0000-0001-5196-609X</orcid>
        <affiliation>
          <orgname>
            3
          </orgname>
        </affiliation>
      </author>
      <author>
        <name>Dmitry Selivanov</name>
        <affiliation>
          <orgname>
            4
          </orgname>
        </affiliation>
      </author>
      <author>
        <name>Jeffrey Arnold</name>
        <orcid>0000-0001-9953-3904</orcid>
        <affiliation>
          <orgname>
            5
          </orgname>
        </affiliation>
      </author>
    </authors>
    <tags>
      <tag>text mining</tag>
      <tag>tokenization</tag>
      <tag>natural language processing</tag>
    </tags>
    <date>12 March 2018</date>
    <paper_doi>10.21105/joss.00655</paper_doi>
    <software_repository>https://github.com/ropensci/tokenizers</software_repository>
    <software_archive>http://dx.doi.org/10.5281/zenodo.1205017</software_archive>
    <paper_url>http://www.theoj.org/joss-papers/joss.00655/10.21105.joss.00655.pdf</paper_url>
  </articleinfo>
  <body>
    <p>Computational text analysis usually proceeds according to a series of well-defined steps. After importing texts, the usual next step is to turn the human-readable text into machine-readable tokens. Tokens are defined as segments of a text identified as meaningful units for the purpose of analyzing the text. They may consist of individual words or of larger or smaller segments, such as word sequences, word subsequences, paragraphs, sentences, or lines <span class="citation" data-cites="Manningetal2008">(Manning, Raghavan, and Schütze 2008, 22)</span>. Tokenization is the process of splitting the text into these smaller pieces, and it often involves preprocessing the text to remove punctuation and transform all tokens into lowercase <span class="citation" data-cites="welbers_text_2017">(Welbers, Van Atteveldt, and Benoit 2017, 250–51)</span>. Decisions made during tokenization have a significant effect on subsequent analysis <span class="citation" data-cites="denny_text_forthcoming guthrie_closer_2006">(Denny and Spirling forthcoming; D. Guthrie et al. 2006)</span>. Especially for large corpora, tokenization can be computationally expensive, and tokenization is highly language dependent. Efficiency and correctness are therefore paramount concerns for tokenization.</p>
    <p>The <a href="http://lincolnmullen.com/software/tokenizers/"><strong>tokenizers</strong></a> package for R provides fast, consistent tokenization for natural language text <span class="citation" data-cites="tokenizers">(L. Mullen 2018, <span class="citation" data-cites="rbase">R Core Team (2017)</span>)</span>. (The package is available on <a href="https://github.com/ropensci/tokenizers">GitHub</a> and archived on <a href="https://doi.org/10.5281/zenodo.1205017">Zenodo</a>.) Each of the tokenizers expects a consistent input and returns a consistent output, so that the tokenizers can be used interchangeably with one another or relied on in other packages. To ensure the correctness of output, the package depends on the stringi package, which implements Unicode support for R <span class="citation" data-cites="gagolewski_2018">(Gagolewski 2018)</span>. To ensure the speed of tokenization, key components such as the <em>n</em>-gram and skip <em>n</em>-gram tokenizers are written using the Rcpp package <span class="citation" data-cites="eddelbuettel_2013">(Eddelbuettel 2013, <span class="citation" data-cites="eddelbuettel_2017">Eddelbuettel and Balamuta (2017)</span>)</span>. The tokenizers package is part of the <a href="https://ropensci.org/">rOpenSci project</a>.</p>
    <p>The most important tokenizers in the current version of the package can be grouped as follows:</p>
    <ul>
    <li>tokenizers for characters and shingled characters</li>
    <li>tokenizers for words and word stems, as well as for Penn Treebank tokens</li>
    <li>tokenizers n-grams and skip n-grams</li>
    <li>tokenizers for tweets, which preserve formatting of usernames and hashtags</li>
    </ul>
    <p>In addition the package provides functions for splitting longer documents into sentences and paragraphs, or for splitting a long text into smaller chunks each with the same number of words. This allows users to treat parts of very long texts as documents in their own right. The package also provides functions for counting words, characters, and sentences.</p>
    <p>The tokenizers in this package can be used on their own, or they can be wrapped by higher-level R packages. For instance, the tokenizers package is a dependency for the tidytext <span class="citation" data-cites="silge_2016">(Silge and Robinson 2016)</span>, text2vec <span class="citation" data-cites="selivanov_2018">(Selivanov and Wang 2018)</span>, and textreuse <span class="citation" data-cites="mullen_2016">(L. Mullen 2016)</span> packages. More broadly, the output of the tokenization functions follows the guidelines set by the text-interchange format defined at an rOpenSci Text Workshop in 2017 <span class="citation" data-cites="tif_2017">(Text Workshop 2017)</span>. Other packages which buy into the text-interchange format can thus use the tokenizers package interchangeably.</p>
    <p>The tokenizers package has research applications in any discipline which uses computational text analysis. The package was originally created for<br />
    historical research into the use of the Bible in American newspapers <span class="citation" data-cites="mullen_americas">(L. A. Mullen forthcoming)</span> and into the borrowing of legal codes of civil procedure in the nineteenth-century United States <span class="citation" data-cites="funkmullen_spine_2018">(Funk and Mullen 2018, <span class="citation" data-cites="funkmullen_servile_2016">Funk and Mullen (2016)</span>)</span>. The <code>tokenizers</code> package underlies the tidytext package <span class="citation" data-cites="silge_text_2017">(Silge and Robinson 2017)</span>, and via that package tokenizers has been used in disciplines such as political science <span class="citation" data-cites="sanger_2015_">(Sanger and Warin, n.d.)</span>, social science <span class="citation" data-cites="warin_mapping">(Warin, Le Duc, and Sanger, n.d.)</span>, communication studies <span class="citation" data-cites="xu_using_2018">(Xu and Guo 2018)</span>, English <span class="citation" data-cites="ballier_rbased_2017">(Ballier and Lissón 2017)</span>, and the digital humanities more generally.</p>
    <h1 id="references" class="unnumbered">References</h1>
    <div id="refs" class="references">
    <div id="ref-ballier_rbased_2017">
    <p>Ballier, Nicolas, and Paula Lissón. 2017. “R-Based Strategies for DH in English Linguistics: A Case Study.” In <em>Teaching NLP for Digital Humanities (Teach4DH) Co-Located with GSCL 2017.</em>, 1918:1–10. Proceedings of the Workshop on Teaching Nlp for Digital Humanities (Teach4dh) Co-Located with Gscl 2017. Berlin, Germany: Peggy Bockwinkel. <a href="https://hal.archives-ouvertes.fr/hal-01587126" class="uri">https://hal.archives-ouvertes.fr/hal-01587126</a>.</p>
    </div>
    <div id="ref-denny_text_forthcoming">
    <p>Denny, Matthew J., and Arthur Spirling. forthcoming. “Text Preprocessing for Unsupervised Learning: Why It Matters, When It Misleads, and What to Do About It.” <em>Political Analysis</em>, 49.</p>
    </div>
    <div id="ref-eddelbuettel_2013">
    <p>Eddelbuettel, Dirk. 2013. <em>Seamless R and C++ Integration with Rcpp</em>. New York: Springer. <a href="https://doi.org/10.1007/978-1-4614-6868-4" class="uri">https://doi.org/10.1007/978-1-4614-6868-4</a>.</p>
    </div>
    <div id="ref-eddelbuettel_2017">
    <p>Eddelbuettel, Dirk, and James Joseph Balamuta. 2017. “Extending extitR with extitC++: A Brief Introduction to extitRcpp.” <em>PeerJ Preprints</em> 5 (August):e3188v1. <a href="https://doi.org/10.7287/peerj.preprints.3188v1" class="uri">https://doi.org/10.7287/peerj.preprints.3188v1</a>.</p>
    </div>
    <div id="ref-funkmullen_servile_2016">
    <p>Funk, Kellen, and Lincoln A. Mullen. 2016. “A Servile Copy: Text Reuse and Medium Data in American Civil Procedure.” <em>Rechtsgeschichte [Legal History]</em>, no. 24:341–43. <a href="https://doi.org/10.12946/rg24/341-343" class="uri">https://doi.org/10.12946/rg24/341-343</a>.</p>
    </div>
    <div id="ref-funkmullen_spine_2018">
    <p>———. 2018. “The Spine of American Law: Digital Text Analysis and U.S. Legal Practice.” <em>American Historical Review</em> 123 (1):132–64. <a href="https://doi.org/10.1093/ahr/123.1.132" class="uri">https://doi.org/10.1093/ahr/123.1.132</a>.</p>
    </div>
    <div id="ref-gagolewski_2018">
    <p>Gagolewski, Marek. 2018. <em>R Package Stringi: Character String Processing Facilities</em>. <a href="http://www.gagolewski.com/software/stringi/" class="uri">http://www.gagolewski.com/software/stringi/</a>.</p>
    </div>
    <div id="ref-guthrie_closer_2006">
    <p>Guthrie, David, Ben Allison, Wei Liu, Louise Guthrie, and Yorick Wilks. 2006. “A Closer Look at Skip-Gram Modelling.” In <em>Proceedings of the 5th International Conference on Language Resources and Evaluation</em>. <a href="http://www.lrec-conf.org/proceedings/lrec2006/pdf/357_pdf.pdf" class="uri">http://www.lrec-conf.org/proceedings/lrec2006/pdf/357_pdf.pdf</a>.</p>
    </div>
    <div id="ref-Manningetal2008">
    <p>Manning, C. D., P. Raghavan, and H. Schütze. 2008. <em>Introduction to Information Retrieval</em>. Cambridge University Press.</p>
    </div>
    <div id="ref-mullen_2016">
    <p>Mullen, Lincoln. 2016. <em>Textreuse: Detect Text Reuse and Document Similarity</em>. <a href="https://github.com/ropensci/textreuse" class="uri">https://github.com/ropensci/textreuse</a>.</p>
    </div>
    <div id="ref-tokenizers">
    <p>———. 2018. <em>Tokenizers: Fast, Consistent Tokenization of Natural Language Text</em>. <a href="http://lincolnmullen.com/software/tokenizers/" class="uri">http://lincolnmullen.com/software/tokenizers/</a>.</p>
    </div>
    <div id="ref-mullen_americas">
    <p>Mullen, Lincoln A. forthcoming. <em>America’s Public Bible: Biblical Quotations in U.S. Newspapers</em>. Stanford University Press. <a href="http://americaspublicbible.org" class="uri">http://americaspublicbible.org</a>.</p>
    </div>
    <div id="ref-rbase">
    <p>R Core Team. 2017. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/" class="uri">https://www.R-project.org/</a>.</p>
    </div>
    <div id="ref-sanger_2015_">
    <p>Sanger, William, and Thierry Warin. n.d. “The 2015 Canadian Election on Twitter: A Tidy Algorithmic Analysis.”</p>
    </div>
    <div id="ref-selivanov_2018">
    <p>Selivanov, Dmitriy, and Qing Wang. 2018. <em>Text2vec: Modern Text Mining Framework for R</em>. <a href="https://CRAN.R-project.org/package=text2vec" class="uri">https://CRAN.R-project.org/package=text2vec</a>.</p>
    </div>
    <div id="ref-silge_2016">
    <p>Silge, Julia, and David Robinson. 2016. “Tidytext: Text Mining and Analysis Using Tidy Data Principles in R.” <em>JOSS</em> 1 (3). The Open Journal. <a href="https://doi.org/10.21105/joss.00037" class="uri">https://doi.org/10.21105/joss.00037</a>.</p>
    </div>
    <div id="ref-silge_text_2017">
    <p>———. 2017. <em>Text Mining with R: A Tidy Approach</em>. O’Reilly. <a href="http://tidytextmining.com/" class="uri">http://tidytextmining.com/</a>.</p>
    </div>
    <div id="ref-tif_2017">
    <p>Text Workshop. 2017. <em>Tif: Text Interchange Format</em>. <a href="https://github.com/ropensci/tif" class="uri">https://github.com/ropensci/tif</a>.</p>
    </div>
    <div id="ref-warin_mapping">
    <p>Warin, Thierry, Romain Le Duc, and William Sanger. n.d. “Mapping Innovations in Artificial Intelligence Through Patents: A Social Data Science Perspective.”</p>
    </div>
    <div id="ref-welbers_text_2017">
    <p>Welbers, Kasper, Wouter Van Atteveldt, and Kenneth Benoit. 2017. “Text Analysis in R.” <em>Communication Methods and Measures</em> 11 (4):245–65. <a href="https://doi.org/10.1080/19312458.2017.1387238" class="uri">https://doi.org/10.1080/19312458.2017.1387238</a>.</p>
    </div>
    <div id="ref-xu_using_2018">
    <p>Xu, Zhan, and Hao Guo. 2018. “Using Text Mining to Compare Online Pro- and Anti-Vaccine Headlines: Word Usage, Sentiments, and Online Popularity.” <em>Communication Studies</em> 69 (1):103–22. <a href="https://doi.org/10.1080/10510974.2017.1414068" class="uri">https://doi.org/10.1080/10510974.2017.1414068</a>.</p>
    </div>
    </div>
  </body>
</article>
