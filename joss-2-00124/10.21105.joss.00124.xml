<?xml version="1.0" encoding="utf-8" ?>
<article>
  <articleinfo>
    <title>Confidence Intervals for Random Forests in Python</title>
    <authors>
      <author>
        <name>Kivan Polimis</name>
        <orcid>0000-0002-3498-0479</orcid>
        <affiliation>
          <orgname>
            1
          </orgname>
        </affiliation>
      </author>
      <author>
        <name>Ariel Rokem</name>
        <orcid>0000-0003-0679-1985</orcid>
        <affiliation>
          <orgname>
            1
          </orgname>
        </affiliation>
      </author>
      <author>
        <name>Bryna Hazelton</name>
        <orcid>0000-0001-7532-645X</orcid>
        <affiliation>
          <orgname>
            1
          </orgname>
        </affiliation>
      </author>
    </authors>
    <tags>
      <tag>Python</tag>
      <tag>scikit-learn</tag>
      <tag>random forest</tag>
      <tag>confidence intervals</tag>
    </tags>
    <date>7 July 2017</date>
    <paper_doi>10.21105/joss.00124</paper_doi>
    <software_repository>https://github.com/scikit-learn-contrib/forest-confidence-interval</software_repository>
    <software_archive>https://doi.org/10.5281/zenodo.1044209</software_archive>
    <paper_url>http://www.theoj.org/joss-papers/joss.00124/10.21105.joss.00124.pdf</paper_url>
  </articleinfo>
  <body>
    <h1 id="summary">Summary</h1>
    <p>Random forests are a method for predicting numerous ensemble learning tasks. Prediction variability can illustrate how influential the training set is for producing the observed random forest predictions and provides additional information about prediction accuracy. <code>forest-confidence-interval</code> is a Python module for calculating variance and adding confidence intervals to <code>scikit-learn</code> random forest regression or classification objects. The core functions calculate an in-bag and error bars for random forest objects. Our software is designed for individuals using <code>scikit-learn</code> random forest objects that want to add estimates of uncertainty to random forest predictors. This module is an implementation of an algorithm developed by <span class="citation">Wager, Hastie, and Efron (2014)</span> and previously implemented in R <span class="citation">(Wager 2016)</span>.</p>
    <h1 id="usage">Usage</h1>
    <p>Our package's <code>random_forest_error</code> and <code>calc_inbag</code> functions use the random forest object (including training and test data) to create variance estimates that can be plotted (e.g. as confidence intervals or standard deviations). The in-bag matrix that fit the data is set to <code>None</code> by default, and the in-bag will be inferred from the forest. However, this only works for trees for which bootstrapping was set to <code>True</code>. That is, if sampling was done with replacement. Otherwise, users need to provide their own in-bag matrix.</p>
    <h1 id="examples-gallery">Examples gallery</h1>
    <p>The regression example uses a slightly modified data-set from the Carnegie Mellon University's StatLib library (available from the <a href="https://archive.ics.uci.edu/ml/datasets/Auto+MPG">UC Irvine Machine Learning Repository</a>) with features of different cars and their MPG <span class="citation">(Quinlan 1993)</span>. The classification example generates synthetic data to simulate a task like that of a spam filter: classifying items into one of two categories (e.g., spam/non-spam) based on a number of features. This module will work for matrices or <code>pandas</code> data frames. Then, <code>scikit-learn</code> functions split the example data into training and test data and generate a random forest object (regression or classifier). The examples calculate variance from random forest objects that use the highest mean probability estimate across the trees. The focus on means for estimates and unit comparability between sample mean and dispersion measures is the basis for plotting with the square root of the variance (standard deviation). As the plots with variance estimated show, some predictions have more error than others. For instance, in the regression (MPG) example, predictions of higher mileage MPG are associated with greater variance than lower mileage predictions.</p>
    <h2 id="regression-example">Regression example</h2>
    <h3 id="no-variance-estimated">No variance estimated</h3>
    <p>-<img src="plot_mpg_no_variance.png" alt="plot-mpg-no-variance" /></p>
    <h3 id="plot-with-variance">Plot with variance</h3>
    <p>-<img src="plot_mpg.png" alt="plot-mpg-variance" /></p>
    <h2 id="classification-example">Classification example</h2>
    <h3 id="no-variance-estimated-1">No variance estimated</h3>
    <p>-<img src="plot_spam_no_variance.png" alt="plot-spam-no-variance" /></p>
    <h3 id="plot-with-variance-1">Plot with variance</h3>
    <p>-<img src="plot_spam.png" alt="plot-spam" /></p>
    <h2 id="community-guidelines">Community guidelines</h2>
    <p>Contributions are very welcome, but we ask that contributors abide by the <a href="http://contributor-covenant.org/version/1/4/">contributor covenant</a>.</p>
    <p>To report issues with the software, please post to the <a href="https://github.com/scikit-learn-contrib/forest-confidence-interval/issues">issue log</a> Bug reports are also appreciated, please add them to the issue log after verifying that the issue does not already exist. Comments on existing issues are also welcome.</p>
    <p>Please submit improvements as pull requests against the repo after verifying that the existing tests pass and any new code is well covered by unit tests. Please write code that complies with the Python style guide, <a href="https://www.python.org/dev/peps/pep-0008/">PEP8</a></p>
    <p>Please e-mail <a href="mailto:arokem@gmail.com">Ariel Rokem</a>, <a href="mailto:kivan.polimis@gmail.com">Kivan Polimis</a>, or <a href="mailto:brynah@phys.washington.edu">Bryna Hazelton</a> if you have any questions, suggestions or feedback.</p>
    <h1 id="references" class="unnumbered">References</h1>
    <div id="refs" class="references">
    <div id="ref-quinlan_combining_1993">
    <p>Quinlan, J. Ross. 1993. “Combining Instance-Based and Model-Based Learning.” In <em>Proceedings of the Tenth International Conference on International Conference on Machine Learning</em>, 236–43. ICML’93. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc. <a href="http://dl.acm.org/citation.cfm?id=3091529.3091560" class="uri">http://dl.acm.org/citation.cfm?id=3091529.3091560</a>.</p>
    </div>
    <div id="ref-wager_randomforestci_2016">
    <p>Wager, Stefan. 2016. “randomForestCI.” <a href="https://github.com/swager/randomForestCI" class="uri">https://github.com/swager/randomForestCI</a>.</p>
    </div>
    <div id="ref-wager_confidence_2014">
    <p>Wager, Stefan, Trevor Hastie, and Bradley Efron. 2014. “Confidence Intervals for Random Forests: The Jackknife and the Infinitesimal Jackknife.” <em>Journal of Machine Learning Research</em> 15 (1): 1625–51. <a href="http://dl.acm.org/citation.cfm?id=2627435.2638587" class="uri">http://dl.acm.org/citation.cfm?id=2627435.2638587</a>.</p>
    </div>
    </div>
  </body>
</article>
