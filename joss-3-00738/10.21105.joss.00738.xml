<?xml version="1.0" encoding="utf-8" ?>
<article>
  <articleinfo>
    <title><code>dit</code>: a Python package for discrete information theory</title>
    <authors>
      <author>
        <name>Ryan G. James</name>
        <orcid>0000-0003-2149-9085</orcid>
        <affiliation>
          <orgname>
            1
          </orgname>
        </affiliation>
      </author>
      <author>
        <name>Christopher J. Ellison</name>
      </author>
      <author>
        <name>James P. Crutchfield</name>
        <affiliation>
          <orgname>
            1
          </orgname>
        </affiliation>
      </author>
    </authors>
    <tags>
      <tag>information theory</tag>
    </tags>
    <date>27 April 2018</date>
    <paper_doi>10.21105/joss.00738</paper_doi>
    <software_repository>https://github.com/dit/dit</software_repository>
    <software_archive>http://dx.doi.org/10.5281/zenodo.1255350</software_archive>
    <paper_url>http://www.theoj.org/joss-papers/joss.00738/10.21105.joss.00738.pdf</paper_url>
  </articleinfo>
  <body>
    <h1 id="summary">Summary</h1>
    <p><code>dit</code><span class="citation" data-cites="dit">(“Dit: A Python Package for Discrete Information Theory. Available at: <a href="https://github.com/dit/dit">Https://Github.com/Dit/Dit</a>” n.d.)</span> is a Python package for the study of <strong>d</strong>iscrete <strong>i</strong>nformation <strong>t</strong>heory. Information theory is a mathematical framework for the study of quantifying, compressing, and communicating random variables <span class="citation" data-cites="Cover2006">(Cover and Thomas 2006)</span><span class="citation" data-cites="MacKay2003">(MacKay 2003)</span><span class="citation" data-cites="Yeung2008">(Yeung 2008)</span>. More recently, information theory has been utilized within the physical and social sciences to quantify how different components of a system interact. <code>dit</code> is primarily concerned with this aspect of the theory.</p>
    <p>Information theory is a powerful extension to probability and statistics, quantifying dependencies among arbitrary random variables in a way tha tis consistent and comparable across systems and scales. Information theory was originally developed to quantify how quickly and reliably information could be transmitted across an arbitrary channel. The demands of modern, data-driven science have been coopting and extending these quantities and methods into unknown, multivariate settings where the interpretation and best practices are not known. For example, there are at least four reasonable multivariate generalizations of the mutual information, none of which inherit all the interpretations of the standard bivariate case. Which is best to use is context-dependent. <code>dit</code> implements a vast range of multivariate information measures in an effort to allow information practitioners to study how these various measures behave and interact in a variety of contexts. We hope that having all these measures and techniques implemented in one place will allow the development of robust techniques for the automated quantification of dependencies within a system and concrete interpretation of what those dependencies mean.</p>
    <p><code>dit</code> implements the vast majority of information measure defined in the literature, including entropies (Shannon<span class="citation" data-cites="Cover2006">(Cover and Thomas 2006)</span>, Renyi, Tsallis), multivariate mutual informations (co-information<span class="citation" data-cites="Bell2003">(Bell 2003)</span><span class="citation" data-cites="mcgill1954multivariate">(McGill 1954)</span>, total correlation<span class="citation" data-cites="watanabe1960information">(Watanabe 1960)</span>, dual total correlation<span class="citation" data-cites="te1980multiple">(Te Sun 1980)</span><span class="citation" data-cites="Han1975linear">(Han 1975)</span><span class="citation" data-cites="Abdallah2012">(Abdallah and Plumbley 2012)</span>, CAEKL mutual information<span class="citation" data-cites="chan2015multivariate">(Chan et al. 2015)</span>), common informations (Gács-Körner<span class="citation" data-cites="Gacs1973">(Gács and Körner 1973)</span><span class="citation" data-cites="tyagi2011function">(Tyagi, Narayan, and Gupta 2011)</span>, Wyner<span class="citation" data-cites="wyner1975common">(Wyner 1975)</span><span class="citation" data-cites="liu2010common">(W. Liu, Xu, and Chen 2010)</span>, exact<span class="citation" data-cites="kumar2014exact">(Kumar, Li, and El Gamal 2014)</span>, functional, minimal sufficient statistic), and channel capacity<span class="citation" data-cites="Cover2006">(Cover and Thomas 2006)</span>. It includes methods of studying joint distributions including information diagrams, connected informations<span class="citation" data-cites="Schneidman2003">(Schneidman et al. 2003)</span><span class="citation" data-cites="Amari2001">(Amari 2001)</span>, marginal utility of information<span class="citation" data-cites="allen2017multiscale">(Allen, Stacey, and Bar-Yam 2017)</span>, and the complexity profile<span class="citation" data-cites="Baryam2004">(Y. Bar-Yam 2004)</span>. It also includes several more specialized modules including bounds on the secret key agreement rate<span class="citation" data-cites="maurer1997intrinsic">(Maurer and Wolf 1997)</span>, partial information decomposition<span class="citation" data-cites="williams2010nonnegative">(Williams and Beer 2010)</span>, rate-distortion theory<span class="citation" data-cites="Cover2006">(Cover and Thomas 2006)</span> &amp; information bottleneck<span class="citation" data-cites="tishby2000information">(Tishby, Pereira, and Bialek 2000)</span>, and others. Please see the <a href="https://github.com/dit/dit"><code>dit</code> homepage</a> for a complete and up-to-date list.</p>
    <p>Where possible, the implementations in <code>dit</code> support multivariate, conditional forms even if not defined that way in the literature. For example, <code>dit</code> implements the multivariate, conditional exact common information even though it was only defined for two variables.</p>
    <h1 id="references" class="unnumbered">References</h1>
    <div id="refs" class="references">
    <div id="ref-Abdallah2012">
    <p>Abdallah, S. A., and M. D. Plumbley. 2012. “A Measure of Statistical Complexity Based on Predictive Information with Application to Finite Spin Systems.” <em>Physics Letters A</em> 376 (4). Elsevier:275–81. <a href="https://doi.org/10.1016/j.physleta.2011.10.066" class="uri">https://doi.org/10.1016/j.physleta.2011.10.066</a>.</p>
    </div>
    <div id="ref-allen2017multiscale">
    <p>Allen, Benjamin, Blake C Stacey, and Yaneer Bar-Yam. 2017. “Multiscale Information Theory and the Marginal Utility of Information.” <em>Entropy</em> 19 (6). Multidisciplinary Digital Publishing Institute:273. <a href="https://doi.org/10.3390/e19060273" class="uri">https://doi.org/10.3390/e19060273</a>.</p>
    </div>
    <div id="ref-Amari2001">
    <p>Amari, Shun-ichi. 2001. “Information Geometry on Hierarchy of Probability Distributions.” <em>Information Theory, IEEE Transactions on</em> 47 (5). IEEE:1701–11. <a href="https://doi.org/10.1109/18.930911" class="uri">https://doi.org/10.1109/18.930911</a>.</p>
    </div>
    <div id="ref-Baryam2004">
    <p>Bar-Yam, Y. 2004. “Multiscale Complexity/Entropy.” <em>Advances in Complex Systems</em> 7 (01). World Scientific:47–63. <a href="https://doi.org/10.1142/S0219525904000068" class="uri">https://doi.org/10.1142/S0219525904000068</a>.</p>
    </div>
    <div id="ref-Bell2003">
    <p>Bell, A. J. 2003. “The Co-Information Lattice.” In <em>Proc. Fifth Intl. Workshop on Independent Component Analysis and Blind Signal Separation</em>, edited by S. Makino S. Amari A. Cichocki and N. Murata, ICA 2003:921–26. New York: Springer.</p>
    </div>
    <div id="ref-chan2015multivariate">
    <p>Chan, Chung, Ali Al-Bashabsheh, Javad B Ebrahimi, Tarik Kaced, and Tie Liu. 2015. “Multivariate Mutual Information Inspired by Secret-Key Agreement.” <em>Proceedings of the IEEE</em> 103 (10). IEEE:1883–1913. <a href="https://doi.org/10.1109/JPROC.2015.2458316" class="uri">https://doi.org/10.1109/JPROC.2015.2458316</a>.</p>
    </div>
    <div id="ref-Cover2006">
    <p>Cover, T. M., and J. A. Thomas. 2006. <em>Elements of Information Theory</em>. Second. New York: Wiley-Interscience. <a href="https://doi.org/10.1002/047174882X" class="uri">https://doi.org/10.1002/047174882X</a>.</p>
    </div>
    <div id="ref-dit">
    <p>“Dit: A Python Package for Discrete Information Theory. Available at: <a href="https://github.com/dit/dit">Https://Github.com/Dit/Dit</a>.” n.d. Accessed April 27, 2018.</p>
    </div>
    <div id="ref-Gacs1973">
    <p>Gács, Peter, and János Körner. 1973. “Common Information Is Far Less Than Mutual Information.” <em>Problems of Control and Information Theory</em> 2 (2):149–62.</p>
    </div>
    <div id="ref-Han1975linear">
    <p>Han, T. S. 1975. “Linear Dependence Structure of the Entropy Space.” <em>Information and Control</em> 29. Elsevier:337–68. <a href="https://doi.org/10.1016/S0019-9958(75)80004-0" class="uri">https://doi.org/10.1016/S0019-9958(75)80004-0</a>.</p>
    </div>
    <div id="ref-kumar2014exact">
    <p>Kumar, G. R., C. T. Li, and A. El Gamal. 2014. “Exact Common Information.” In <em>Information Theory (Isit), 2014 Ieee International Symposium on</em>, 161–65. IEEE. <a href="https://doi.org/10.1109/ISIT.2014.6874815" class="uri">https://doi.org/10.1109/ISIT.2014.6874815</a>.</p>
    </div>
    <div id="ref-liu2010common">
    <p>Liu, Wei, Ge Xu, and Biao Chen. 2010. “The Common Information of N Dependent Random Variables.” In <em>Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Conference on</em>, 836–43. IEEE. <a href="https://doi.org/10.1109/ALLERTON.2010.5706995" class="uri">https://doi.org/10.1109/ALLERTON.2010.5706995</a>.</p>
    </div>
    <div id="ref-MacKay2003">
    <p>MacKay, D. J.C. 2003. <em>Information Theory, Inference and Learning Algorithms</em>. Cambridge university press.</p>
    </div>
    <div id="ref-maurer1997intrinsic">
    <p>Maurer, Ueli, and Stefan Wolf. 1997. “The Intrinsic Conditional Mutual Information and Perfect Secrecy.” In <em>IEEE International Symposium on Information Theory</em>, 88–88. Citeseer. <a href="https://doi.org/10.1109/ISIT.1997.613003" class="uri">https://doi.org/10.1109/ISIT.1997.613003</a>.</p>
    </div>
    <div id="ref-mcgill1954multivariate">
    <p>McGill, W. J. 1954. “Multivariate Information Transmission.” <em>Psychometrika</em> 19 (2). Springer:97–116. <a href="https://doi.org/10.1109/TIT.1954.1057469" class="uri">https://doi.org/10.1109/TIT.1954.1057469</a>.</p>
    </div>
    <div id="ref-Schneidman2003">
    <p>Schneidman, E., S. Still, M. J. Berry, W. Bialek, and others. 2003. “Network Information and Connected Correlations.” <em>Phys. Rev. Lett.</em> 91 (23). APS:238701. <a href="https://doi.org/10.1103/PhysRevLett.91.238701" class="uri">https://doi.org/10.1103/PhysRevLett.91.238701</a>.</p>
    </div>
    <div id="ref-te1980multiple">
    <p>Te Sun, H. 1980. “Multiple Mutual Informations and Multiple Interactions in Frequency Data.” <em>Information and Control</em> 46. Elsevier:26–45. <a href="https://doi.org/10.1016/S0019-9958(80)90478-7" class="uri">https://doi.org/10.1016/S0019-9958(80)90478-7</a>.</p>
    </div>
    <div id="ref-tishby2000information">
    <p>Tishby, Naftali, Fernando C Pereira, and William Bialek. 2000. “The Information Bottleneck Method.” <em>arXiv Preprint Physics/0004057</em>.</p>
    </div>
    <div id="ref-tyagi2011function">
    <p>Tyagi, H., P. Narayan, and P. Gupta. 2011. “When Is a Function Securely Computable?” <em>Information Theory, IEEE Transactions on</em> 57 (10). IEEE:6337–50. <a href="https://doi.org/10.1109/TIT.2011.2165807" class="uri">https://doi.org/10.1109/TIT.2011.2165807</a>.</p>
    </div>
    <div id="ref-watanabe1960information">
    <p>Watanabe, S. 1960. “Information Theoretical Analysis of Multivariate Correlation.” <em>IBM Journal of Research and Development</em> 4 (1). IBM:66–82. <a href="https://doi.org/10.1147/rd.41.0066" class="uri">https://doi.org/10.1147/rd.41.0066</a>.</p>
    </div>
    <div id="ref-williams2010nonnegative">
    <p>Williams, Paul L, and Randall D Beer. 2010. “Nonnegative Decomposition of Multivariate Information.” <em>arXiv Preprint arXiv:1004.2515</em>.</p>
    </div>
    <div id="ref-wyner1975common">
    <p>Wyner, A. D. 1975. “The Common Information of Two Dependent Random Variables.” <em>Information Theory, IEEE Transactions on</em> 21 (2). IEEE:163–79. <a href="https://doi.org/10.1109/TIT.1975.1055346" class="uri">https://doi.org/10.1109/TIT.1975.1055346</a>.</p>
    </div>
    <div id="ref-Yeung2008">
    <p>Yeung, R. W. 2008. <em>Information Theory and Network Coding</em>. Springer. <a href="https://doi.org/10.1007/978-0-387-79234-7" class="uri">https://doi.org/10.1007/978-0-387-79234-7</a>.</p>
    </div>
    </div>
  </body>
</article>
