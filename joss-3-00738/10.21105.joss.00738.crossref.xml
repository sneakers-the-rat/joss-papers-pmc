<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/4.4.0" xmlns:ai="http://www.crossref.org/AccessIndicators.xsd" xmlns:rel="http://www.crossref.org/relations.xsd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="4.4.0" xsi:schemaLocation="http://www.crossref.org/schema/4.4.0 http://www.crossref.org/schemas/crossref4.4.0.xsd">
  <head>
    <doi_batch_id>dedf2a5459e7a5f0d2402f2687f62731</doi_batch_id>
    <timestamp>20180531055302</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>http://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>05</month>
          <year>2018</year>
        </publication_date>
        <journal_volume>
          <volume>3</volume>
        </journal_volume>
        <issue>25</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>dit: a Python package for discrete information theory</title>
        </titles>
        <contributors><person_name sequence="first" contributor_role="author"><given_name>Ryan</given_name><surname>G. James</surname><ORCID>http://orcid.org/0000-0003-2149-9085</ORCID></person_name><person_name sequence="additional" contributor_role="author"><given_name>Christopher</given_name><surname>J. Ellison</surname></person_name><person_name sequence="additional" contributor_role="author"><given_name>James</given_name><surname>P. Crutchfield</surname></person_name></contributors>
        <publication_date>
          <month>05</month>
          <day>31</day>
          <year>2018</year>
        </publication_date>
        <pages>
          <first_page>738</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.00738</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">http://dx.doi.org/10.5281/zenodo.1255350</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/738</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.00738</doi>
          <resource>http://joss.theoj.org/papers/10.21105/joss.00738</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">http://www.theoj.org/joss-papers/joss.00738/10.21105.joss.00738.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list><citation key="ref1"><unstructured_citation>New York, Cover, Thomas M. and Thomas, Joy A., Second, 0471241954, 776, Wiley-Interscience, Elements of Information Theory, 2006</unstructured_citation></citation><citation key="ref2"><unstructured_citation>Information theory, inference and learning algorithms, MacKay, David JC, 2003, Cambridge university press</unstructured_citation></citation><citation key="ref3"><unstructured_citation>Information theory and network coding, Yeung, Raymond W, 2008, Springer</unstructured_citation></citation><citation key="ref4"><unstructured_citation>Calude, Cristian, Information and Randomness: An Algorithmic Perspective, 2002, 3540434666, 2nd, Springer-Verlag New York, Inc., Secaucus, NJ, USA</unstructured_citation></citation><citation key="ref5"><unstructured_citation>Bell, A. J., The Co-information Lattice, Proc. Fifth Intl. Workshop on Independent Component Analysis and Blind Signal Separation, S. Amari, A. Cichocki, S. Makino and Murata, N., Springer, New York, ICA 2003, , 921–926, 2003</unstructured_citation></citation><citation key="ref6"><unstructured_citation>Multivariate information transmission, McGill, W. J., Psychometrika, 19, 2, 97–116, 1954, Springer</unstructured_citation></citation><citation key="ref7"><unstructured_citation>Information theoretical analysis of multivariate correlation, Watanabe, S., IBM Journal of research and development, 4, 1, 66–82, 1960, IBM</unstructured_citation></citation><citation key="ref8"><unstructured_citation>Multiple mutual informations and multiple interactions in frequency data, Te Sun, H., Information and Control, 46, 26–45, 1980, Elsevier</unstructured_citation></citation><citation key="ref9"><unstructured_citation>Linear dependence structure of the entropy space, Han, T. S., Information and Control, 29, 337–368, 1975, Elsevier</unstructured_citation></citation><citation key="ref10"><unstructured_citation>A measure of statistical complexity based on predictive information with application to finite spin systems, Abdallah, S. A. and Plumbley, M. D., Physics Letters A, 376, 4, 275–281, 2012, Elsevier</unstructured_citation></citation><citation key="ref11"><unstructured_citation>Erasure entropy, Verdú, S. and Weissman, T., Information Theory, 2006 IEEE International Symposium on, 98–102, 2006, IEEE</unstructured_citation></citation><citation key="ref12"><unstructured_citation>The information lost in erasures, Verdu, Sergio and Weissman, Tsachy, Information Theory, IEEE Transactions on, 54, 11, 5030–5058, 2008, IEEE</unstructured_citation></citation><citation key="ref13"><unstructured_citation>Multivariate Mutual Information Inspired by Secret-Key Agreement, Chan, Chung and Al-Bashabsheh, Ali and Ebrahimi, Javad B and Kaced, Tarik and Liu, Tie, Proceedings of the IEEE, 103, 10, 1883–1913, 2015, IEEE</unstructured_citation></citation><citation key="ref14"><unstructured_citation>Common information is far less than mutual information, Gács, Peter and Körner, János, Problems of Control and Information Theory, 2, 2, 149–162, 1973</unstructured_citation></citation><citation key="ref15"><unstructured_citation>When is a function securely computable?, Tyagi, H. and Narayan, P. and Gupta, P., Information Theory, IEEE Transactions on, 57, 10, 6337–6350, 2011, IEEE</unstructured_citation></citation><citation key="ref16"><unstructured_citation>The common information of two dependent random variables, Wyner, A. D., Information Theory, IEEE Transactions on, 21, 2, 163–179, 1975, IEEE</unstructured_citation></citation><citation key="ref17"><unstructured_citation>The common information of N dependent random variables, Liu, Wei and Xu, Ge and Chen, Biao, Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Conference on, 836–843, 2010, IEEE</unstructured_citation></citation><citation key="ref18"><unstructured_citation>Exact common information, Kumar, G. R. and Li, C. T. and El Gamal, A., Information Theory (ISIT), 2014 IEEE International Symposium on, 161–165, 2014, IEEE</unstructured_citation></citation><citation key="ref19"><unstructured_citation>Extropy: a complementary dual of entropy, Lad, Frank and Sanfilippo, Giuseppe and Agrò, Gianna, arXiv preprint arXiv:1109.6440, 2011</unstructured_citation></citation><citation key="ref20"><unstructured_citation>A measure for brain complexity: relating functional segregation and integration in the nervous system, Tononi, Giulio and Sporns, Olaf and Edelman, Gerald M, Proceedings of the National Academy of Sciences, 91, 11, 5033–5037, 1994, National Acad Sciences</unstructured_citation></citation><citation key="ref21"><unstructured_citation>Cumulative residual entropy: a new measure of information, Rao, Murali and Chen, Yunmei and Vemuri, Baba C and Wang, Fei, Information Theory, IEEE Transactions on, 50, 6, 1220–1228, 2004, IEEE</unstructured_citation></citation><citation key="ref22"><unstructured_citation>An information-theoretic formalism for multiscale structure in complex systems, Allen, B. and Stacey, B. C. and Bar-Yam, Y., arXiv preprint arXiv:1409.4708, 2014</unstructured_citation></citation><citation key="ref23"><unstructured_citation>Multiscale complexity/entropy, Bar-Yam, Y., Advances in Complex Systems, 7, 01, 47–63, 2004, World Scientific</unstructured_citation></citation><citation key="ref24"><unstructured_citation>Information geometry on hierarchy of probability distributions, Amari, Shun-ichi, Information Theory, IEEE Transactions on, 47, 5, 1701–1711, 2001, IEEE</unstructured_citation></citation><citation key="ref25"><unstructured_citation>Network information and connected correlations, Schneidman, E. and Still, S. and Berry, M. J. and Bialek, W. and others, Phys. Rev. Lett., 91, 23, 238701, 2003, APS</unstructured_citation></citation><citation key="ref26"><unstructured_citation>Intensive entropic non-triviality measure, Lamberti, P. W. and Martin, M. T. and Plastino, A. and Rosso, O. A., Physica A: Statistical Mechanics and its Applications, 334, 1, 119–131, 2004, Elsevier</unstructured_citation></citation><citation key="ref27"><unstructured_citation>The Multivariate Entropy Triangle and Applications, Valverde-Albacete, Francisco José and Peláez-Moreno, Carmen, Hybrid Artificial Intelligent Systems, 647–658, 2016, Springer</unstructured_citation></citation><citation key="ref28"><unstructured_citation>Coordination capacity, Cuff, Paul Warner and Permuter, Haim H and Cover, Thomas M, IEEE Transactions on Information Theory, 56, 9, 4181–4206, 2010, IEEE</unstructured_citation></citation><citation key="ref29"><unstructured_citation>Lautum information, Palomar, Daniel P and Verdú, Sergio, IEEE transactions on information theory, 54, 3, 964–975, 2008, IEEE</unstructured_citation></citation><citation key="ref30"><unstructured_citation>Phi-Entropic Measures of Correlation, Beigi, Salman and Gohari, Amin, arXiv preprint arXiv:1611.01335, 2016</unstructured_citation></citation><citation key="ref31"><unstructured_citation>The intrinsic conditional mutual information and perfect secrecy, Maurer, Ueli and Wolf, Stefan, IEEE international symposium on information theory, 88–88, 1997, Citeseer</unstructured_citation></citation><citation key="ref32"><unstructured_citation>A property of the intrinsic mutual information, Christandl, Matthias and Renner, Renato and Wolf, Stefan, IEEE international symposium on information theory, 258–258, 2003</unstructured_citation></citation><citation key="ref33"><unstructured_citation>A new measure for conditional mutual information and its properties, Renner, Renato and Skripsky, Juraj and Wolf, Stefan, IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, 259–259, 2003</unstructured_citation></citation><citation key="ref34"><unstructured_citation>Comments On “Information-Theoretic Key Agreement of Multiple Terminals—Part I”, Gohari, Amin and Anantharam, Venkat, IEEE Transactions on Information Theory, 63, 8, 5440–5442, 2017, IEEE</unstructured_citation></citation><citation key="ref35"><unstructured_citation>On Achieving a Positive Rate in the Source Model Key Agreement Problem, Gohari, Amin and Günlü, Onur and Kramer, Gerhard, arXiv preprint arXiv:1709.05174, 2017</unstructured_citation></citation><citation key="ref36"><unstructured_citation>The information bottleneck method, Tishby, Naftali and Pereira, Fernando C and Bialek, William, arXiv preprint physics/0004057, 2000</unstructured_citation></citation><citation key="ref37"><unstructured_citation>Nonnegative decomposition of multivariate information, Williams, Paul L and Beer, Randall D, arXiv preprint arXiv:1004.2515, 2010</unstructured_citation></citation><citation key="ref38"><unstructured_citation>Bivariate measure of redundant information, Harder, Malte and Salge, Christoph and Polani, Daniel, Physical Review E, 87, 1, 012130, 2013, APS</unstructured_citation></citation><citation key="ref39"><unstructured_citation>Quantifying unique information, Bertschinger, Nils and Rauh, Johannes and Olbrich, Eckehard and Jost, Jürgen and Ay, Nihat, Entropy, 16, 4, 2161–2183, 2014, Multidisciplinary Digital Publishing Institute</unstructured_citation></citation><citation key="ref40"><unstructured_citation>Quantifying synergistic mutual information, Griffith, Virgil and Koch, Christof, Guided Self-Organization: Inception, 159–190, 2014, Springer</unstructured_citation></citation><citation key="ref41"><unstructured_citation>Computing the Unique Information, Banerjee, Pradeep Kr. and Rauh, Johannes and Montúfar, Guido, arXive preprint arXiv:1709.07487, 2017</unstructured_citation></citation><citation key="ref42"><unstructured_citation>Intersection information based on common randomness, Griffith, Virgil and Chong, Edwin KP and James, Ryan G and Ellison, Christopher J and Crutchfield, James P, Entropy, 16, 4, 1985–2000, 2014, Multidisciplinary Digital Publishing Institute</unstructured_citation></citation><citation key="ref43"><unstructured_citation>Measuring multivariate redundant information with pointwise common change in surprisal, Ince, Robin A. A., Entropy, 19, 7, 318, 2017, Multidisciplinary Digital Publishing Institute</unstructured_citation></citation><citation key="ref44"><unstructured_citation>Unique Information via Dependency Constraints, James, Ryan G. and Emenheiser, Jeffrey and Crutchfield, James P., arXiv preprint arXiv:1709.06653, 2017</unstructured_citation></citation><citation key="ref45"><unstructured_citation>, Finn, Conor and Lizier, Joseph, , 2017</unstructured_citation></citation><citation key="ref46"><unstructured_citation>Temporal information partitioning: Characterizing synergy, uniqueness, and redundancy in interacting environmental variables, Goodwell, Allison E and Kumar, Praveen, Water Resources Research, 53, 7, 5920–5942, 2017, Wiley Online Library</unstructured_citation></citation><citation key="ref47"><unstructured_citation>The Partial Entropy Decompostion: decomposing multivariate entropy and mutual information via pointwise common surprisal, Ince, Robin A. A., arXive preprint arXiv:1702.01591, 2017</unstructured_citation></citation><citation key="ref48"><unstructured_citation>Shared information—new insights and problems in decomposing information in complex systems, Bertschinger, Nils and Rauh, Johannes and Olbrich, Eckehard and Jost, Jürgen, Proceedings of the European Conference on Complex Systems 2012, 251–269, 2013, Springer</unstructured_citation></citation><citation key="ref49"><unstructured_citation>On extractable shared information, Rauh, Johannes and Banerjee, Pradeep Kr and Olbrich, Eckehard and Jost, Jürgen and Bertschinger, Nils, arXiv preprint arXiv:1701.07805, 2017</unstructured_citation></citation><citation key="ref50"><unstructured_citation>Reconsidering unique information: Towards a multivariate information decomposition, Rauh, Johannes and Bertschinger, Nils and Olbrich, Eckehard and Jost, Jurgen, Information Theory (ISIT), 2014 IEEE International Symposium on, 2232–2236, 2014, IEEE</unstructured_citation></citation><citation key="ref51"><unstructured_citation>Redundancy and synergy in dual decompositions of mutual information gain and information loss, Chicharro, Daniel and Panzeri, Stefano, arXiv preprint arXiv:1612.09522, 2016</unstructured_citation></citation><citation key="ref52"><unstructured_citation>Secret Sharing and Shared Information, Rauh, Johannes, arXiv preprint arXiv:1706.06998, 2017</unstructured_citation></citation><citation key="ref53"><unstructured_citation>Synergy, redundancy and common information, Banerjee, Pradeep Kr and Griffith, Virgil, arXiv preprint arXiv:1509.03706, 2015</unstructured_citation></citation><citation key="ref54"><unstructured_citation>Information decomposition and synergy, Olbrich, Eckehard and Bertschinger, Nils and Rauh, Johannes, Entropy, 17, 5, 3501–3517, 2015, Multidisciplinary Digital Publishing Institute</unstructured_citation></citation><citation key="ref55"><unstructured_citation>Towards a synergy-based approach to measuring information modification, Lizier, Joseph T and Flecker, Benjamin and Williams, Paul L, Artificial Life (ALIFE), 2013 IEEE Symposium on, 43–51, 2013, IEEE</unstructured_citation></citation><citation key="ref56"><unstructured_citation>Generalized measures of information transfer, Williams, Paul L and Beer, Randall D, arXiv preprint arXiv:1102.1507, 2011</unstructured_citation></citation><citation key="ref57"><unstructured_citation>Ross Ashby’s information theory: a bit of history, some solutions to problems, and what we face today, Krippendorff, Klaus, International Journal of General Systems, 38, 2, 189–212, 2009, Taylor &amp; Francis</unstructured_citation></citation><citation key="ref58"><unstructured_citation>Understanding interdependency through complex information sharing, Rosas, Fernando and Ntranos, Vasilis and Ellison, Christopher J and Pollin, Sofie and Verhelst, Marian, Entropy, 18, 2, 38, 2016, Multidisciplinary Digital Publishing Institute</unstructured_citation></citation><citation key="ref59"><unstructured_citation>Invariant components of synergy, redundancy, and unique information among three variables, Pica, Giuseppe and Piasini, Eugenio and Chicharro, Daniel and Panzeri, Stefano, arXiv preprint arXiv:1706.08921, 2017</unstructured_citation></citation><citation key="ref60"><unstructured_citation>Information encryption in the expert management of strategic uncertainty, Frey, Seth and Williams, Paul L and Albino, Dominic K, arXiv preprint arXiv:1605.04233, 2016</unstructured_citation></citation></citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
