<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/4.4.0" xmlns:ai="http://www.crossref.org/AccessIndicators.xsd" xmlns:rel="http://www.crossref.org/relations.xsd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="4.4.0" xsi:schemaLocation="http://www.crossref.org/schema/4.4.0 http://www.crossref.org/schemas/crossref4.4.0.xsd">
  <head>
    <doi_batch_id>778156cedbaa4d2c6f92865f102731c2</doi_batch_id>
    <timestamp>20171102062800</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>The Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>http://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>11</month>
          <year>2017</year>
        </publication_date>
        <journal_volume>
          <volume>2</volume>
        </journal_volume>
        <issue>19</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>pyGPGO: Bayesian Optimization for Python</title>
        </titles>
        <contributors><person_name sequence="first" contributor_role="author"><given_name>José</given_name><surname>Jiménez</surname><ORCID>http://orcid.org/0000-0002-5335-7834</ORCID></person_name><person_name sequence="additional" contributor_role="author"><given_name>Josep</given_name><surname>Ginebra</surname><ORCID>http://orcid.org/0000-0001-9521-9635</ORCID></person_name></contributors>
        <publication_date>
          <month>11</month>
          <day>02</day>
          <year>2017</year>
        </publication_date>
        <pages>
          <first_page>431</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.00431</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">https://doi.org/10.5281/zenodo.1040676</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/openjournals/joss-reviews/issues/431</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.00431</doi>
          <resource>http://joss.theoj.org/papers/10.21105/joss.00431</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">http://www.theoj.org/joss-papers/joss.00431/10.21105.joss.00431.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list><citation key="ref1"><doi>10.1109/JPROC.2015.2494218</doi></citation><citation key="ref2"><doi>2012arXiv1206.2944S</doi></citation><citation key="ref3"><doi>10.1162/153244303322533223</doi></citation><citation key="ref4"><unstructured_citation>Spearmint, Snoek, Jasper, Spearmint, https://github.com/HIPS/Spearmint, 2012</unstructured_citation></citation><citation key="ref5"><unstructured_citation>scikit-optimize, scikit-optimize team., The, scikit-optimize, https://github.com/scikit-optimize/scikit-optimize, 2016</unstructured_citation></citation><citation key="ref6"><unstructured_citation>MOE, Yelp, MOE, https://github.com/Yelp/MOE, 2014</unstructured_citation></citation><citation key="ref7"><unstructured_citation> We propose a novel information-theoretic approach\backslashnfor Bayesian optimization called Predictive Entropy\backslashnSearch (PES). At each iteration, PES selects the\backslashnnext evaluation point that maximizes the expected\backslashninformation gained with respect to the global\backslashnmaximum. PES codifies this intractable acquisition\backslashnfunction in terms of the expected reduction in the\backslashndifferential entropy of the predictive distribution.\backslashnThis reformulation allows PES to obtain\backslashnapproximations that are both more accurate and\backslashnefficient than other alternatives such as Entropy\backslashnSearch (ES). Furthermore, PES can easily perform a\backslashnfully Bayesian treatment of the model\backslashnhyperparameters while ES cannot. We evaluate PES in\backslashnboth synthetic and realworld applications, including\backslashnoptimization problems in machine learning, finance,\backslashnbiotechnology, and robotics. We show that the\backslashnincreased accuracy of PES leads to significant gains\backslashnin optimization performance. , arXiv, arXiv:1406.2541v1, Hernández-Lobato, José Miguel and Hoffman, Matthew W and Ghahramani, Zoubin, arXiv:1406.2541v1, 10495258, Advances in Neural Information Processing Systems 28, 1–9, Predictive Entropy Search for Efficient Global Optimization of Black-box Functions, https://jmhldotorg.files.wordpress.com/2014/10/pes-final.pdf, 2014</unstructured_citation></citation><citation key="ref8"><unstructured_citation>Bayesian optimization is a powerful frame- work for minimizing expensive objective functions while using very few function eval- uations. It has been successfully applied to a variety of problems, including hyperparam- eter tuning and experimental design. How- ever, this framework has not been extended to the inequality-constrained optimization setting, particularly the setting in which eval- uating feasibility is just as expensive as eval- uating the objective. Here we present con- strained Bayesian optimization, which places a prior distribution on both the objective and the constraint functions. We evaluate our method on simulated and real data, demon- strating that constrained Bayesian optimiza- tion can quickly find optimal and feasible points, even when small feasible regions cause standard methods to fail., Gardner, Jacob R. and Kusner, Matt J. and Xu, Zhixiang Eddie and Weinberger, Kilian Q. and Cunningham, John P., 9781634393973, Proceedings of the 31st International Conference on Machine Learning, 937–945, Bayesian Optimization with Inequality Constraints, 32, 2014</unstructured_citation></citation><citation key="ref9"><unstructured_citation>Thompson sampling is one of oldest heuristic to address the exploration ex- ploitation trade-off, but it is surprisingly unpopular in the literature. We present here some empirical results using Thompson sampling on simulated and real data, and show that it is highly competitive. And since this heuristic is very easy to implement, we argue that it should be part of the standard baselines to compare against., Chapelle, Olivier and Li, Lihong, 9781618395993, Advances in Neural Information Processing Systems, 22492257, An Empirical Evaluation of Thompson Sampling, http://explo.cs.ucl.ac.uk/wp-content/uploads/2011/05/An-Empirical-Evaluation-of-Thompson-Sampling-Chapelle-Li-2011.pdf, 2011</unstructured_citation></citation><citation key="ref10"><doi>10.1145/2645710.2645733</doi></citation><citation key="ref11"><unstructured_citation>Probabilistic programming allows for automatic Bayesian inference on user-defined probabilistic models. Recent advances in Markov chain Monte Carlo (MCMC) sampling allow inference on increasingly complex models. This class of MCMC, known as Hamiltonian Monte Carlo, requires gradient information which is often not readily available. PyMC3 is a new open source probabilistic programming framework written in Python that uses Theano to compute gradients via automatic differentiation as well as compile probabilistic programs on-the-fly to C for increased speed. Contrary to other probabilistic programming languages, PyMC3 allows model specification directly in Python code. The lack of a domain specific language allows for great flexibility and direct interaction with the model. This paper is a tutorial-style introduction to this software package., Salvatier, J and Wiecki, TV and C., Fonnesbeck, PeerJ Computer Science, Probabilistic programming in Python using PyMC3, https://doi.org/10.7717/peerj-cs.55, 2016</unstructured_citation></citation><citation key="ref12"><doi>10.1142/S0129065704001899</doi></citation></citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
