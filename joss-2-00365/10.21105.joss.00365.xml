<?xml version="1.0" encoding="utf-8" ?>
<article>
  <articleinfo>
    <title>gmm_diag and gmm_full: C++ classes for multi-threaded Gaussian mixture models and Expectation-Maximisation</title>
    <authors>
      <author>
        <name>Conrad Sanderson</name>
        <orcid>0000-0002-0049-4501</orcid>
        <affiliation>
          <orgname>
            1, 2, 4
          </orgname>
        </affiliation>
      </author>
      <author>
        <name>Ryan Curtin</name>
        <orcid>0000-0002-9903-8214</orcid>
        <affiliation>
          <orgname>
            3, 4
          </orgname>
        </affiliation>
      </author>
    </authors>
    <tags>
      <tag>modelling</tag>
      <tag>statistics</tag>
      <tag>numerics</tag>
    </tags>
    <date>12 October 2017</date>
    <paper_doi>10.21105/joss.00365</paper_doi>
    <software_repository>https://github.com/conradsnicta/armadillo-gmm</software_repository>
    <software_archive>http://dx.doi.org/10.5281/zenodo.1012627</software_archive>
    <paper_url>https://github.com/openjournals/joss-papers/raw/master/joss.00365/10.21105.joss.00365.pdf</paper_url>
  </articleinfo>
  <body>
    <h1 id="summary">Summary</h1>
    <p>Statistical modelling of multivariate data through a convex mixture of Gaussians, also known as a Gaussian mixture model (GMM), has many applications in fields such as signal processing, econometrics, and pattern recognition <span class="citation">(Bishop 2006)</span>. Each component (Gaussian) in a GMM is parameterised with a weight, mean vector (centroid), and covariance matrix.</p>
    <p><em>gmm_diag</em> and <em>gmm_full</em> are C++ classes which provide multi-threaded (parallelised) implementations of GMMs and the associated Expectation-Maximisation (EM) algorithm for learning the underlying parameters from training data <span class="citation">(Moon 1996)</span><span class="citation">(McLachlan and Krishnan 2008)</span>. The <em>gmm_diag</em> class is specifically tailored for diagonal covariance matrices (all entries outside the main diagonal in each covariance matrix are assumed to be zero), while the <em>gmm_full</em> class is tailored for full covariance matrices. The <em>gmm_diag</em> class is typically much faster to train and use than the <em>gmm_full</em> class, at the potential cost of some reduction in modelling accuracy.</p>
    <p>The interface for the <em>gmm_diag</em> and <em>gmm_full</em> classes allows the user easy and flexible access to the trained model, as well as control over the underlying parameters. Specifically, the two classes contain functions for likelihood evaluation, vector quantisation, histogram generation, data synthesis, and parameter modification, in addition to training (learning) the GMM parameters via the EM algorithm. The classes use several techniques to improve numerical stability and promote convergence of EM based training, such as keeping as much as possible of the internal computations in the log domain, and ensuring the covariance matrices stay positive-definite.</p>
    <p>To achieve multi-threading, the EM training algorithm has been reformulated into a MapReduce-like framework <span class="citation">(Dean and Ghemawat 2008)</span> and implemented with the aid of OpenMP pragma directives <span class="citation">(Chapman, Jost, and Pas 2007)</span>. As such, the EM algorithm runs much quicker on multi-core machines when OpenMP is enabled during compilation (for example, using the <code>-fopenmp</code> option in GCC and clang compilers).</p>
    <p>The <em>gmm_diag</em> and <em>gmm_full</em> classes are included in version 7.960 of the Armadillo C++ library <span class="citation">(Sanderson and Curtin 2016)</span>, with the underlying mathematical details described in <span class="citation">(Sanderson and Curtin 2017)</span>. The documentation for the classes is available online<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>.</p>
    <h1 id="references" class="unnumbered">References</h1>
    <div id="refs" class="references">
    <div id="ref-Bishop_2006">
    <p>Bishop, Christopher. 2006. <em>Pattern Recognition and Machine Learning</em>. Springer.</p>
    </div>
    <div id="ref-OpenMP_2007">
    <p>Chapman, Barbara, Gabriele Jost, and Ruud van der Pas. 2007. <em>Using OpenMP: Portable Shared Memory Parallel Programming</em>. MIT Press.</p>
    </div>
    <div id="ref-MapReduce_2008">
    <p>Dean, Jeffrey, and Sanjay Ghemawat. 2008. “MapReduce: Simplified Data Processing on Large Clusters.” <em>Communications of the ACM</em> 51 (1): 107–13. doi:<a href="https://doi.org/10.1145/1327452.1327492">10.1145/1327452.1327492</a>.</p>
    </div>
    <div id="ref-McLachlan_2008">
    <p>McLachlan, Geoffrey, and Thriyambakam Krishnan. 2008. <em>The EM Algorithm and Extensions</em>. 2nd ed. John Wiley &amp; Sons.</p>
    </div>
    <div id="ref-Moon96">
    <p>Moon, Todd K. 1996. “Expectation-Maximization Algorithm.” <em>IEEE Signal Processing Magazine</em> 13 (6): 47–60. doi:<a href="https://doi.org/10.1109/79.543975">10.1109/79.543975</a>.</p>
    </div>
    <div id="ref-Armadillo_2016">
    <p>Sanderson, Conrad, and Ryan Curtin. 2016. “Armadillo: A Template-Based C++ Library for Linear Algebra.” <em>Journal of Open Source Software</em> 1: 26. doi:<a href="https://doi.org/10.21105/joss.00026">10.21105/joss.00026</a>.</p>
    </div>
    <div id="ref-Arxiv_1707_09094">
    <p>———. 2017. “An Open Source C++ Implementation of Multi-Threaded Gaussian Mixture Models, K-Means and Expectation Maximisation.” <em>arXiv:1707.09094</em>.</p>
    </div>
    </div>
    <div class="footnotes">
    <hr />
    <ol>
    <li id="fn1"><p>http://arma.sourceforge.net/docs.html#gmm_diag<a href="#fnref1">↩</a></p></li>
    </ol>
    </div>
  </body>
</article>
