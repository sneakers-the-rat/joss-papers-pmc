<?xml version="1.0" encoding="utf-8" ?>
<article>
  <articleinfo>
    <title>Nostril: A nonsense string evaluator written in Python</title>
    <authors>
      <author>
        <name>Michael Hucka</name>
        <orcid>0000-0001-9105-5960</orcid>
        <affiliation>
          <orgname>
            1
          </orgname>
        </affiliation>
      </author>
    </authors>
    <tags>
      <tag>mining source repositories</tag>
      <tag>identifiers</tag>
      <tag>text processing</tag>
      <tag>inference</tag>
    </tags>
    <date>6 February 2018</date>
    <paper_doi>10.21105/joss.00596</paper_doi>
    <software_repository>https://github.com/casics/nostril</software_repository>
    <software_archive>http://dx.doi.org/10.22002/D1.935</software_archive>
    <paper_url>http://www.theoj.org/joss-papers/joss.00596/10.21105.joss.00596.pdf</paper_url>
  </articleinfo>
  <body>
    <h1 id="summary">Summary</h1>
    <p>Nostril (<em>Nonsense String Evaluator</em>) is a Python 3 module that can infer whether a given word or text string is likely to be nonsense or meaningful text. A “meaningful” string of characters is one constructed from real or real-looking English words or fragments of real words (even if the words are <em>runtogetherlikethis</em>). The main use case for Nostril is to decide whether short strings returned by source code mining methods are likely to be program identifiers (of classes, functions, variables, etc.), or random or other non-identifier strings.</p>
    <p>Nostril is easy to use. It provides a Python function named <code>nonsense()</code>; this function takes a single text string as an argument and returns a Boolean value as a result. Here is an example of its use. The following code,</p>
    <div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1">    <span class="im">from</span> nostril <span class="im">import</span> nonsense</a>
    <a class="sourceLine" id="cb1-2" data-line-number="2">    <span class="cf">for</span> s <span class="kw">in</span> [<span class="st">&#39;bunchofwords&#39;</span>, <span class="st">&#39;xywinlist&#39;</span>, <span class="st">&#39;faiwtlwexu&#39;</span>, <span class="st">&#39;asfgtqwafazfy&#39;</span>]:</a>
    <a class="sourceLine" id="cb1-3" data-line-number="3">        <span class="cf">if</span> nonsense(s):</a>
    <a class="sourceLine" id="cb1-4" data-line-number="4">           <span class="bu">print</span>(<span class="st">&quot;</span><span class="sc">{}</span><span class="st"> is nonsense&quot;</span>.<span class="bu">format</span>(s))</a>
    <a class="sourceLine" id="cb1-5" data-line-number="5">        <span class="cf">else</span>:</a>
    <a class="sourceLine" id="cb1-6" data-line-number="6">           <span class="bu">print</span>(<span class="st">&quot;</span><span class="sc">{}</span><span class="st"> is real&quot;</span>.<span class="bu">format</span>(s))</a></code></pre></div>
    <p>produces the following output:</p>
    <pre><code>    bunchofwords is real
        xywinlist is real
        faiwtlwexu is nonsense
        asfgtqwafazfy is nonsense</code></pre>
    <p>Nostril also includes a command-line program named <code>nostril</code>; it will evaluate strings provided on the command line or in a file, and is useful for experimenting with Nostril or using it in command-oriented workflows.</p>
    <h1 id="the-need-for-detecting-nonsense">The need for detecting nonsense</h1>
    <p>A number of research efforts have investigated extracting and analyzing textual information contained in software artifacts <span class="citation" data-cites="Dit2011-fz Linstead2009-ky">(e.g., Dit et al. 2011; Linstead et al. 2009)</span>. However, source code files can contain meaningless text, such as random text used as markers or test cases, and code extraction methods can also sometimes make mistakes and produce garbled text. When used in processing pipelines without human intervention, it is often important to include a data cleaning step before passing tokens extracted from source code to subsequent analysis or machine learning algorithms. Thus, a basic (and often unmentioned) step is to filter out nonsense tokens.</p>
    <p>Discerning real identifiers from nonsense is a surprisingly difficult problem, because program identifiers often consist of words, acronyms and word fragments jammed together (e.g., <code>ioFlXFndrInfo</code>). The resulting strings can challenge even humans. Nostril uses a combination of (1) a prefilter that detects simple positive and negative cases using heuristic rules and (2) a custom TF-IDF <span class="citation" data-cites="manning2009introduction">(Manning, Raghavan, and Schütze 2009)</span> scoring scheme that uses letter 4-grams as features. The software includes a precomputed table of n-gram weights derived by training the system on a large set of strings constructed from concatenated American English words, real text corpora, and other inputs. Parameter values were optimized using the evolutionary algorithm NSGA-II <span class="citation" data-cites="deb2000fast">(Deb et al. 2000)</span>.</p>
    <p>By default, Nostril is tuned to reduce false positives – it is more likely to say something is <em>not</em> gibberish when it really might be. This bias is motivated by Nostril’s original purpose of filtering source code identifiers for machine-learning applications, where false positives would cause real identifiers to be filtered out and potentially-useful features to be missed. However, the bias and other parameters (and the table of n-grams) can also be retrained if applications require it.</p>
    <p>Nostril is reasonably fast: once the package is loaded, a string evaluation takes 30–50 microseconds on average on a 4 Ghz Apple macOS computer. Nostril is accurate: it achieves 99.76% on the the Ludiso identifier oracle <span class="citation" data-cites="binkley2013dataset">(Binkley et al. 2013)</span> and 91.70% on a test set of 1,000,000 machine-generated random strings.</p>
    <h1 id="acknowledgments">Acknowledgments</h1>
    <p>This material is based upon work supported by the <a href="https://nsf.gov">National Science Foundation</a> under Grant Number 1533792. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p>
    <h1 id="references" class="unnumbered">References</h1>
    <div id="refs" class="references">
    <div id="ref-binkley2013dataset">
    <p>Binkley, David, Dawn Lawrie, Lori Pollock, Emily Hill, and K Vijay-Shanker. 2013. “A Dataset for Evaluating Identifier Splitters.” In <em>Proceedings of the 10th Working Conference on Mining Software Repositories</em>, 401–4. IEEE Press. <a href="http://dl.acm.org/citation.cfm?id=2487085.2487158" class="uri">http://dl.acm.org/citation.cfm?id=2487085.2487158</a>.</p>
    </div>
    <div id="ref-deb2000fast">
    <p>Deb, Kalyanmoy, Samir Agrawal, Amrit Pratap, and T Meyarivan. 2000. “A Fast Elitist Non-Dominated Sorting Genetic Algorithm for Multi-Objective Optimization: NSGA-II.” In <em>Parallel Problem Solving from Nature PPSN VI</em>, 849–58. Springer. <a href="https://doi.org/10.1007/3-540-45356-3_83" class="uri">https://doi.org/10.1007/3-540-45356-3_83</a>.</p>
    </div>
    <div id="ref-Dit2011-fz">
    <p>Dit, B, L Guerrouj, D Poshyvanyk, and G Antoniol. 2011. “Can Better Identifier Splitting Techniques Help Feature Location?” In <em>2011 IEEE 19th International Conference on Program Comprehension</em>, 11–20. <a href="https://doi.org/10.1109/ICPC.2011.47" class="uri">https://doi.org/10.1109/ICPC.2011.47</a>.</p>
    </div>
    <div id="ref-Linstead2009-ky">
    <p>Linstead, Erik, Sushil Bajracharya, Trung Ngo, Paul Rigor, Cristina Lopes, and Pierre Baldi. 2009. “Sourcerer: Mining and Searching Internet-Scale Software Repositories.” <em>Data Mining and Knowledge Discovery</em> 18 (2):300–336. <a href="https://doi.org/10.1007/s10618-008-0118-x" class="uri">https://doi.org/10.1007/s10618-008-0118-x</a>.</p>
    </div>
    <div id="ref-manning2009introduction">
    <p>Manning, Christopher D, Prabhakar Raghavan, and Hinrich Schütze. 2009. <em>Introduction to Information Retrieval</em>. [Online edition]. Cambridge University Press. <a href="https://doi.org/10.1017/CBO9780511809071" class="uri">https://doi.org/10.1017/CBO9780511809071</a>.</p>
    </div>
    </div>
  </body>
</article>
